{
  "items": [
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/aws-glue-250-entity-types-50-countries/",
      "title": "  AWS Glue now can detect 250 sensitive entity types from over 50 countries ",
      "summary": "AWS Glue's sensitive data detection feature now supports over 250 sensitive entity types from 50 countries, allowing customers to easily identify and redact sensitive data without complex coding.",
      "content": " Posted On:  Jun 23, 2023 \nSensitive data detection feature in AWS Glue can now detect over 250 sensitive entity types from 50 countries out-of-the-box.\nSensitive data detection feature in AWS Glue identifies a variety of sensitive data elements like social security numbers, credit card numbers, names, driver license numbers and other entities. Once detected, customers can take actions to redact the sensitive information before writing records into their data repositories. Customers can use this from AWS Glue Studio, Glue Interactive sessions and AWS Glue Studio.\nWith this new capability, customers can detect data that is classified as sensitive in over 50 countries without having to write complex code. Supported countries include Argentina, Australia, Austria, Balkans, Belgium, Brazil, Bulgaria, Canada, Chile, China, Colombia, Croatia, Cyprus, Czechia, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Iceland, India, Indonesia, Ireland, Israel, Italy, Japan, Korea, Latvia, Liechtenstein, Lithuania, Luxembourg, Malaysia, Malta, Mexico, Netherlands, New Zealand, Norway, Philippines, Poland, Portugal, Romania, Singapore, Slovakia, Slovenia, SouthAfrica, Spain, Sri Lanka, Sweden, Switzerland, Thailand, Turkey, UAE, United Kingdom, Ukraine, United States, and Venezuela. Customers can also create custom detectors to detect entities specific to their organizations. This feature is available in the commercial Regions as AWS Glue.\nTo learn more about Sensitive Data Detection in AWS Glue Studio, visit our documentation.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/aws-step-functions-versions-aliases/",
      "title": "  AWS Step Functions launches Versions and Aliases ",
      "summary": "AWS Step Functions introduces Versions and Aliases to enable resilient deployments of serverless workflows, supporting continuous deployment and safe rollbacks.",
      "content": " Posted On:  Jun 22, 2023 \nAWS Step Functions announces the availability of Versions and Aliases, improving resiliency for deployments of serverless workflows. AWS Step Functions is a visual workflow service capable of orchestrating over 11,000+ API actions from over 250 AWS services to automate business processes and data processing workloads.\nNow, AWS Step Functions supports more resilient deployments with Versions and Aliases for workflows, a new set of capabilities that makes it easier for you to set up continuous deployment, to help you iterate faster and release safely into production. Using Step Functions Versions and Aliases you can maintain multiple versions of your workflows, track which version was used for each execution, and create aliases that route traffic between workflow versions. You can deploy your workflows gradually using industry standard techniques such as blue-green and canary style deployments with fast rollbacks to your Step Functions workflows, increasing deployment safety and reducing downtime and risk.\u00a0\nThere is no additional fee for Versions and Aliases, so you only pay for what you use as per existing AWS Step Functions pricing. Please visit Step Functions pricing to learn more.\nYou can get started using Versions and Aliases in the AWS console, AWS CloudFormation, the AWS Command Line Interface (CLI), or the AWS Cloud Development Kit (CDK). To learn more, please see the AWS Step Functions Developer Guide and the Launch Blog to get started.\nAWS Step Functions Versions and Aliases is available in the regions listed here. For a complete list of regions and service offerings, see AWS Regions.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/aws-amplify-ui-builder-figma-plugin/",
      "title": "  Announcing the AWS Amplify UI Builder Figma plugin ",
      "summary": "AWS Amplify announces UI Builder Figma plugin, enabling design-dev collaboration, theming, and code generation from Figma designs to accelerate frontend development.",
      "content": " Posted On:  Jun 22, 2023 \nAWS Amplify announces the UI Builder Figma plugin, empowering design and development teams to seamlessly collaborate within a Figma file. Use this plugin with the Amplify UI kit to easily theme your components, upgrade to new UI kit versions, and generate and preview React code from your designs directly in Figma. \nGo from design to code in seconds by generating clean React code directly inside Figma and see a live preview of the code running before adding it to your application. Open the code in Codesandbox for easy editing and reviewing or copy the code and add to your React application to get pixel perfect frontend code. \nThe AWS Amplify UI Builder Figma plugin and Amplify UI kit are available to everyone. To get started:\nTo learn more about building full-stack web and mobile apps faster with AWS Amplify, visit our website. \n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/aws-lambda-starting-timestamp-kafka-sources/",
      "title": "  AWS Lambda supports starting from timestamp for Kafka event sources ",
      "summary": "AWS Lambda now supports starting from a specific timestamp when using Amazon MSK or Self-Managed Kafka as an event source, enabling precise starting points for processing messages.",
      "content": " Posted On:  Jun 21, 2023 \nAWS Lambda now supports starting from a specific timestamp when using Amazon Managed Streaming for Apache Kafka (MSK) or Self-Managed Kafka as an event source. Previously, Kafka event source mappings could only have starting positions of trim horizon or latest. Now with starting from a timestamp, you can start processing messages at a precise point in time. This is useful for situations like Disaster Recovery, where you need a new consumer to quickly start processing where you previously left off.\nWhen a Kafka event source mapping is configured to start from a specific timestamp, the event source mapping will start processing messages in a topic at or the first message after the specified timestamp. To use this feature, create a new Kafka event source mapping, set the StartingPosition to AT_TIMESTAMP, and set StartingPositionTimestamp to the desired starting position. The StartingPositionTimestamp needs to be formatted in Unix time seconds.\u00a0Please note that in Kafka, the starting position is only used for a new consumer group or when an existing consumer group points to an offset that is invalid (expires). New Kafka event source mappings will generate a new consumer group ID if not otherwise configured with a specific consumer group ID.\nThis feature incurs no additional charge. You pay for the Lambda invocations triggered by the event source mapping connected to Kafka. To learn more, see the Lambda Developer Guide for Amazon MSK and Apache Kafka.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-clouwatch-logs-insights-dedup-command/",
      "title": "  Amazon CloudWatch Logs announces new Log Insights dedup command  ",
      "summary": "Amazon CloudWatch Logs introduces a new Logs Insights command 'dedup' to eliminate duplicate results when analyzing logs, simplifying log analysis.",
      "content": " Posted On:  Jun 20, 2023 \nAmazon CloudWatch Logs is excited to announce a new Logs Insights command, dedup, which enables customers to eliminate duplicate results when analyzing logs. Customers frequently want to query their logs and view only unique results based on one or more fields. You can now use the new dedup command in your Amazon CloudWatch Logs Insights queries to view unique results based on one or more fields. For example, you can view the most recent error message for each hostname by executing the dedup command on the hostname field.\nAmazon CloudWatch Logs Insights is a fully integrated, interactive, pay-as-you-go log analytics service for CloudWatch. Logs Insights enables you to explore, analyze, and visualize your logs, allowing you to troubleshoot operational problems with ease. Logs Insights supports queries with aggregations, filters, and regular expressions for analyzing your logs. Customers can now analyze results relevant to their investigation by using the Log Insights dedup command to remove duplicates.\nThis feature is now supported in all AWS Regions where CloudWatch Logs is available.\u00a0\nTo learn more about the new dedup command, visit the CloudWatch Logs Query Syntax Guide or select \u201cQuery help\u201d from within the CloudWatch Log Insights console page. With a few clicks in the AWS Management Console, you can start using CloudWatch Logs Insights to query logs sent to CloudWatch. To get started with CloudWatch Log Insights, you can visit our getting started guide and the pricing page to learn more.\u00a0 \u00a0\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-dynamodb-cost-failed-conditional-writes/",
      "title": "  Amazon DynamoDB now simplifies and lowers the cost of handling failed conditional writes ",
      "summary": "DynamoDB now provides a copy of the item during failed conditional writes, simplifying error handling without extra read operations, available across AWS services and SDKs.",
      "content": " Posted On:  Jun 30, 2023 \nAmazon DynamoDB now simplifies and lowers the cost of handling failed conditional writes by providing a copy of the item as it was during the failed write attempt. This lets you easily determine the cause of the condition error and then respond to failed conditional writes without having to perform a separate read operation to retrieve the item.\nPreviously, condition check errors in single write operations did not return a copy of the item in the event of a condition check error. A separate read request was necessary to get the item and investigate the cause of the error. Now with the ReturnValuesOnConditionCheckFailure parameter, DynamoDB error messages can include a copy of the item as it was during the write attempt at no additional cost.\nThe new parameter is available in all AWS Regions and supported in all the AWS SDKs, the DynamoDB APIs, the AWS CLI, and PartiQL for DynamoDB. To get started, add the parameter to your PutItem, UpdateItem, or DeleteItem operations and set the value to ALL_OLD.\u00a0To learn more about condition checks, please see the following page.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-s3-restore-status-s3-glacier-objects-list-api/",
      "title": "  Amazon S3 provides restore status of S3 Glacier objects using the S3 LIST API ",
      "summary": "Amazon S3 now provides restore status of objects in S3 Glacier through the S3 LIST API, enabling applications to identify and access restored objects from low-cost S3 Glacier storage.",
      "content": " Posted On:  Jun 29, 2023 \nAmazon S3 now provides the restore status of objects stored in Amazon S3 Glacier Flexible Retrieval and Amazon S3 Glacier Deep Archive using the S3 LIST API. This new information in S3 LIST API responses can be integrated into your applications, helping you identify and access restored objects from S3 Glacier. For example, S3 Glacier restore status is now integrated into Amazon Athena, so you can run queries against your restored data from low-cost S3 Glacier storage classes.\nTo get started, use the S3 LIST API with the new optional header x-amz-optional-object-attributes: RestoreStatus. For any ongoing or recently restored object, the S3 LIST response will show restore status either as in progress or complete. If the restore is already complete, you will also see the expiry date that was specified in your restore request.\nAmazon S3 LIST API support for restore status of objects in S3 Glacier is available in all commercial AWS Regions, including the AWS GovCloud (US) Regions, the AWS China (Beijing) Region, operated by Sinnet, and the AWS China (Ningxia) Region, operated by NWCD.\nYou can use the AWS SDK, API, or CLI to get the S3 Glacier restore status of objects using the S3 List API. To learn more about S3 LIST API, visit the documentation.\u00a0\u00a0\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/aws-database-migration-service-premigration-assessments/",
      "title": "  AWS Database Migration Service now provides more comprehensive premigration assessments ",
      "summary": "AWS DMS expands premigration assessments to provide more detailed recommendations on migration settings and best practices, helping identify and fix issues before migration.",
      "content": " Posted On:  Jun 30, 2023 \nAWS Database Migration Service (AWS DMS) has\u00a0expanded the functionality of premigration assessments. A premigration assessment evaluates the source and target databases of a database migration task to help identify any problems that might prevent a migration from running as expected. By identifying and fixing issues before a migration starts, you can avoid delays in completing the database migration.\nWith this enhancement, the premigration assessments can obtain more detailed information about the source schema and tables to provide recommendations on the AWS DMS settings that should be used. For example, the assessment can suggest which method of reading redo logs for change data capture (CDC) should be used or it could check if the recommended settings have been enabled, providing best practice recommendations from AWS DMS experts.\u00a0\nTo learn more, see Enabling and working with premigration assessments for a task.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-cloudwatch-dashboard-variables/",
      "title": "  Amazon CloudWatch now supports dashboard variables ",
      "summary": "Amazon CloudWatch introduces dashboard variables, enabling easier navigation between different operational views and efficient management of dashboards by switching between data sets.",
      "content": " Posted On:  Jun 30, 2023 \nWe are excited to announce Amazon CloudWatch dashboard variables, a new experience that makes it easier for you to quickly navigate between different operational views by using variables as navigation to view different data sets for the same resource. \nWith dashboard variables, you can create drill-down filters that enable you to build multiple views within a single dashboard. You can create a single dashboard with multiple custom variables, where you can switch between data sets, and more efficiently reuse and manage a more efficient fleet of dashboards.\nCustom labels become navigation on your dashboard that can be used to toggle between data sets in the same view. You can now define set of labels such as 'application environment', 'region' or a 'customer id' and switch between those data sets based on the selection of label by using dropdown selectors, radio buttons or input boxes.\nThe new dashboard variable experience is now available in all AWS commercial regions at no additional cost and you can start using it immediately. You can use a step by step guide in the CloudWatch custom dashboard user interface to configure and manage your variables, or directly from a CloudWatch custom dashboard JSON, by adding an array of \"variables\" to the dashboard definition.\nTo learn more about creating dashboard variables, please refer to our documentation.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-simple-email-service-metric-export/",
      "title": "  Amazon Simple Email Service now supports metric export ",
      "summary": "Amazon SES now allows exporting key deliverability metrics through Virtual Deliverability Manager's dashboard, enabling custom analysis and integration with other data sources to optimize email performance.",
      "content": " Posted On:  Jun 29, 2023 \nNow you can export key deliverability metrics in Amazon Simple Email Service (SES) through Virtual Deliverability Manager's (VDM) dashboard. This makes it easier to export performance indicators such as delivery volume, complaint rate, and click rate into analysis tools such as spreadsheets for custom analysis.\nVDM is an SES feature that helps you enhance email deliverability, like increasing inbox deliverability and email conversions, by providing insights into your sending and delivery data, and giving advice on how to fix the issues that are negatively affecting your delivery success rate and reputation. The VDM dashboard provides insights into your deliverability data focusing on account, ISP, sending identity, and configuration set levels. Exported metrics provide direct access to deliverability metrics in CSV format, giving you flexibility to perform a range of activities such as building your own analyses of deliverability performance with tools such as Excel, or creating detailed graphs of historic deliverability performance for presentations. This can also help you integrate deliverability metrics with other data sources for more detailed analysis, to support troubleshooting or optimization investigations.\nSES supports export in all AWS regions where SES is available.\u00a0\nFor more information, see the VDM dashboard documentation.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/aws-amplify-hosting-monorepo-frameworks/",
      "title": "  AWS Amplify Hosting announces support for monorepo frameworks ",
      "summary": "AWS Amplify Hosting now supports monorepo frameworks like npm workspaces, Yarn workspaces, pnpm workspaces, Turborepo and Nx, enabling zero-configuration deploys for apps in a monorepo.",
      "content": " Posted On:  Jun 29, 2023 \nToday, AWS Amplify Hosting announces monorepo framework support for npm workspaces, Yarn workspaces, pnpm workspaces, Turborepo and Nx. With this release, AWS Amplify Hosting offers fully managed CI/CD deployments and hosting for apps contained within a monorepo (aka monorepository, multi-package repository, multi-project repository, or monolithic repository).\nIn addition to supporting popular monorepo frameworks, AWS Amplify Hosting automatically applies build settings for apps in a npm workspace, Yarn workspace or Nx, enabling zero-configuration deploys.\nAWS Amplify Hosting and its improved support for monorepo frameworks is generally available in the following 19 AWS Regions: US East (Ohio), US East (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific (Hong Kong), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Canada (Central), Europe (Frankfurt), Europe (Stockholm), Europe (Milan), Europe (Ireland), Europe (London), Europe (Paris), Middle East (Bahrain) and South America (S\u00e3o Paulo).\nTo learn more about using Next.js with Nx and AWS Amplify Hosting , visit our launch blog.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-athena-querying-restored-data-s3-glacier/",
      "title": "  Amazon Athena now supports querying restored data in S3 Glacier ",
      "summary": "Amazon Athena now supports querying data stored in Amazon S3 Glacier storage classes, enabling direct analysis of restored data without copying it to Standard storage.",
      "content": " Posted On:  Jun 29, 2023 \nWe are excited to announce that you can now use Amazon Athena to query data stored in Amazon Simple Storage Service (S3) Glacier storage classes.\u00a0S3 Glacier is a secure and durable service for low-cost data archiving and long-term backup. With this launch, you can use Athena to directly query restored data in the Glacier Flexible Retrieval and Deep Archive storage classes, saving you time by removing the need to move and duplicate data.\nAthena is a serverless, interactive analytics service that makes it simple to analyze petabyte-scale data in your S3 data lake and 30 data sources. With today's launch, you can query restored data in S3 Glacier Flexible Retrieval and Deep Archive storage classes without copying data to the Standard storage class for use cases such as log analysis and long-term trend analysis. For example, you can conduct log analysis across storage classes in a single query by combining recent logs stored in S3 Standard with restored older logs in S3 Glacier Flexible Retrieval. You can enable S3 Glacier access for queries in Athena SQL workgroups.\nThis feature is available with Athena Engine V3 in all Amazon Athena supported regions. To learn more about querying S3 Glacier data in Athena, see Querying restored Glacier objects.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-aurora-mysql-zero-etl-integration-redshift-public-preview/",
      "title": "  AWS announces Amazon Aurora MySQL zero-ETL integration with Amazon Redshift (Public Preview) ",
      "summary": "Amazon Aurora MySQL now integrates with Amazon Redshift for near real-time analytics and ML on transactional data, enabling faster insights without complex data pipelines.",
      "content": " Posted On:  Jun 28, 2023 \nAmazon Aurora MySQL zero-ETL integration with Amazon Redshift is now available in public preview. This feature enables near real-time analytics and machine learning (ML) on petabytes of transactional data stored in Amazon Aurora MySQL-Compatible Edition. Data written into Aurora is available in Amazon Redshift within seconds, so you can quickly act on it without having to build and maintain complex data pipelines. Amazon Aurora MySQL zero-ETL integration with Amazon Redshift is now available for Amazon Aurora Serverless v2 and Provisioned as well as Amazon Redshift Serverless and RA3 instance types. \nZero-ETL integration helps you derive holistic insights across many applications, and break data silos in your organization, by allowing you to analyze data from multiple Aurora database clusters in an Amazon Redshift instance. Enhance your data analysis with the rich analytics capabilities of Amazon Redshift, such as high performance SQL, built-in ML and Spark integrations, materialized views, data sharing, and direct access to multiple data stores and data lakes.\u00a0 \nAmazon Aurora MySQL zero-ETL integration with Amazon Redshift is now available in public preview for the\u00a0latest Aurora MySQL 3 version (compatible with MySQL 8) and higher in the US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), and Europe (Ireland) Regions. \nTo learn more\u00a0and get started with a zero-ETL integration, visit the getting started guides for Aurora and Amazon Redshift.\u00a0\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/new-odbc-driver-amazon-athena/",
      "title": "  New ODBC driver now available for Amazon Athena ",
      "summary": "Amazon Athena released a new ODBC driver with improved connectivity, authentication, and query result access features for SQL development and BI applications.",
      "content": " Posted On:  Jun 29, 2023 \nToday, Amazon Athena released\u00a0a new ODBC driver that improves the experience of connecting to, querying, and visualizing data from your favorite SQL development and business intelligence applications.\nThe new version of Athena\u2019s ODBC driver supports the features of the existing driver, is simple to upgrade, and includes new features such as support for authenticating users\u00a0through AWS IAM Identity Center and\u00a0the option to read query results from Amazon S3, which may make query results available to you sooner.\nTo learn more, see Connecting to Amazon Athena with ODBC in the Athena user guide.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-athena-aws-middle-east-uae-region/",
      "title": "  Amazon Athena is now available in the AWS Middle East (UAE) Region ",
      "summary": "Amazon Athena, a serverless analytics service, is now available in the AWS Middle East (UAE) Region, expanding its reach in the Middle East.",
      "content": " Posted On:  Jun 29, 2023 \nWith today\u2019s release, Amazon Athena and its latest features and benefits are available in the AWS Middle East (UAE) Region. This release expands Athena\u2019s availability in the Middle East to include Bahrain and the UAE.\nAmazon Athena is a serverless, interactive analytics service built on open-source frameworks, supporting open-table and file formats. Athena provides a simplified, flexible way to analyze petabytes of data. Athena is built on open-source Trino and Presto engines, with no provisioning or configuration effort required. \nFor a complete list of AWS services available in this and other regions, refer to the AWS Regional Services List.\nTo learn more, see Amazon Athena.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-aurora-serverless-v2-melbourne/",
      "title": "  Amazon Aurora Serverless v2 is now available in Asia Pacific (Melbourne) ",
      "summary": "Amazon Aurora Serverless v2 is now available in the Asia Pacific (Melbourne) region, providing on-demand, automatic scaling for demanding applications without the need to manage database capacity.",
      "content": " Posted On:  Jun 28, 2023 \nAmazon Aurora Serverless v2, the next version of Aurora Serverless, is now available in the Asia Pacific (Melbourne).\nAurora Serverless is an on-demand, automatic scaling configuration for Amazon Aurora. Aurora Serverless v2 scales instantly to support even the most demanding applications. It adjusts capacity in fine-grained increments to provide just the right amount of database resources for an application\u2019s needs. You don\u2019t need to manage database capacity, and you only pay for the resources consumed by your application.\nAurora Serverless v2 provides the full breadth of Amazon Aurora capabilities. Amazon Aurora Serverless v2 is ideal for a broad set of applications. For example, enterprises that have hundreds of thousands of applications, or software as a service (SaaS) vendors that have multi-tenant environments with hundreds or thousands of databases, can use Aurora Serverless v2 to manage database capacity across the entire fleet.\nAurora Serverless v2 is available for the MySQL 8.0-, PostgreSQL 13-, PostgreSQL 14- and PostgreSQL 15-compatible editions of Amazon Aurora. For pricing details and Region availability, visit Amazon Aurora Pricing. To learn more, read the documentation, and get started by creating an Aurora Serverless v2 database using only a few steps in the AWS Management Console.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/aws-systems-manager-parameter-store-api-limit/",
      "title": "  AWS Systems Manager Parameter Store increases API throughput limit ",
      "summary": "AWS Parameter Store now supports up to 10,000 transactions per second for the GetParameter API, making it easier to use for high-traffic applications without performance issues.",
      "content": " Posted On:  Jul 3, 2023 \nParameter Store, a capability of AWS Systems Manager, now supports up to 10,000 transactions per second (TPS) for the\u00a0GetParameter\u00a0API, increased from the previous 3,000 TPS limit. Parameter Store allows you to securely store configuration data and secrets as hierarchical key-value pairs. You can flexibly store parameters such as API keys, subnet IDs, and passwords, and you can reference those parameters in your code and through AWS services such as AWS Lambda, Amazon EC2, and AWS CloudFormation. This increased API limit makes it easier to use Parameter Store to support high-traffic applications and demanding workloads without sacrificing performance due to API throttling.\nThe 10,000 TPS limit is available when you enable the higher throughput setting in your Parameter Store account. If you already have the higher throughput setting enabled, you can start using 10,000 TPS today with no additional steps required.\nTo turn on higher throughput for Parameter Store, visit the documentation. For information about pricing, visit the Systems Manager Pricing page. This feature is available in all AWS Regions where AWS Systems Manager is available.\n9/8/2023: 10,000 API transactions per second is only supported for the GetParameter API. To confirm API limits for other APIs such as GetParameters and GetParametersByPath, see\u00a0AWS Systems Manager endpoints and quotas.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-cloudwatch-cross-account-service-quotas/",
      "title": "  Amazon CloudWatch now supports Cross-Account Service Quotas ",
      "summary": "CloudWatch now supports tracking service quotas across multiple AWS accounts, allowing centralized monitoring and visualization of resource utilization and limits.",
      "content": " Posted On:  Jul 5, 2023 \nAmazon CloudWatch now supports Service Quotas in Cross-Account observability allowing customers to track and visualize resource utilization and limits across various AWS services from multiple AWS accounts within a region using a central monitoring account.\nQuotas are the limits or the maximum values for the resources, actions, and items of the AWS service in your AWS account. Using Cross-Account Service Quotas, customers can view these quotas for all their source accounts from a single monitoring account and prevent hitting the limits. Customers no longer have to track the quotas by logging into individual accounts, instead from a central monitoring account, they can create dashboards and alarms for the AWS service quota usage across all their source accounts.\nCross-Account Service Quotas is now available in commercial Amazon Web Services Regions, including the Amazon Web Services China (Beijing) Region, operated by Sinnet, and the Amazon Web Services China (Ningxia) Region, operated by NWCD.\nTo get started, you first need to setup CloudWatch cross-account observability. Once configuration is complete, please refer to Service Quota visualization documentation for further information. Service Quotas are available at no additional charge. Standard pricing applies for the APIs, Alarms and Dashboard, see Amazon CloudWatch pricing page for details.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-opensearch-service-provision-iops-throughput-gp3-volumes/",
      "title": "  Amazon OpenSearch Service now lets you provision higher IOPS and throughput for gp3 volumes ",
      "summary": "Amazon OpenSearch Service now allows provisioning up to 16,000 IOPS and 1000 MiB/s throughput per 3 TiB gp3 volume size per data node, improving search and indexing performance.",
      "content": " Posted On:  Jul 5, 2023 \nAmazon OpenSearch Service now lets you provision up to 16,000 IOPS and 1000 MiB/s throughput for every 3 TiB gp3 volume size provisioned per data node, enabling you to achieve better search and indexing performance. \nPreviously, customers were allowed to provision a maximum of 16,000 IOPS and up to 1000 MiB/s throughput per data node for their gp3 volume type, irrespective of the provisioned volume size. These provisioning limits worked for the majority of workloads. However, a few workloads, especially those with large data volumes or high search traffic, required the ability to provision higher IOPS and throughput to address their performance bottlenecks. Now, you can provision up to 16,000 IOPS and 1000 MiB/s throughput for every 3 TiB volume size provisioned per data node. Please note that your instance type can limit the maximum IOPS and throughput provisioning. You can check the maximum limits supported for an instance type in the EBS documentation.\nThe increased IOPS and throughput limits for gp3 volumes are now available on Amazon OpenSearch Service domains in all AWS regions globally where Amazon OpenSearch service is available. Please refer to the AWS Region Table for more information about Amazon OpenSearch Service.\nFor more information regarding making configuration changes in the Amazon OpenSearch Service, please see the documentation. \nTo learn more about Amazon OpenSearch Service, please visit the product page.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/aws-backup-cross-account-aws-region-coverage/",
      "title": "  AWS Backup expands cross-account backup AWS Region coverage ",
      "summary": "AWS Backup expands cross-account backup coverage to more regions, allowing customers to copy backups across accounts for additional protection against disruptions or data loss.",
      "content": " Posted On:  Jul 5, 2023 \nToday, AWS Backup is announcing expanded regional coverage for cross-account backups in AWS Regions Africa (Cape Town), Asia Pacific (Hong Kong, Hyderabad, Jakarta, Melbourne, Osaka), Europe (Milan, Spain, Zurich), and Middle East (Bahrain, UAE). Using cross-account backups, customers can copy backups across accounts within their AWS Organizations.\nWith cross-account backup in AWS Backup, customers can copy data from the backup vault of the source account to the backup vaults of destination accounts, either on-demand or as part of a scheduled backup plan. These copies provide customers an additional layer of protection should the source account experience disruption from accidental or malicious deletion, disaster, or ransomware. If your original backup is inadvertently deleted, you can copy the backup from its destination account to its source account, and then start the restore.\nAWS Backup supports cross-account backup in US East (N Virginia, Ohio), US West (North California, Oregon), Africa (Cape Town), Asia Pacific (Hong Kong, Hyderabad, Jakarta, Mumbai, Osaka, Seoul, Singapore, Sydney, Tokyo), Canada (Central), Europe (Frankfurt, Ireland, London, Milan, Paris, Spain, Stockholm, Zurich), Middle East (Bahrain, UAE), South America (Sao Paulo), and the AWS GovCloud (US-East, US-West) Regions. For more information on AWS Backup cross-account backups, visit the documentation. Get started with AWS Backup today.\n"
    },
    {
      "link": "https://aws.amazon.com/blogs/aws/new-solution-clickstream-analytics-on-aws-for-mobile-and-web-applications/",
      "title": "New Solution \u2013 Clickstream Analytics on AWS for Mobile and Web Applications",
      "summary": "Clickstream Analytics on AWS is a solution that allows you to capture, ingest, store, analyze, and visualize your customers' clickstreams within your web and mobile applications, while keeping your data secure in your AWS account.",
      "content": "Starting today, you can deploy on your AWS account an end-to-end solution to capture, ingest, store, analyze, and visualize your customers\u2019 clickstreams inside your web and mobile applications (both for Android and iOS). The solution is built on top of standard AWS services.\nThis new solution Clickstream Analytics on AWS allows you to keep your data in the security and compliance perimeter of your AWS account and customize the processing and analytics as you require, giving you the full flexibility to extract value for your business. For example, many business line owners want to combine clickstream analytics data with business system data to gain more comprehensive insights. Storing clickstream analysis data in your AWS account allows you to cross reference the data with your existing business system, which is complex to implement when you use a third-party analytics solution that creates an artificial data silo.\nClickstream Analytics on AWS\u00a0is available from the AWS Solutions Library at no cost, except for the services it deploys on your account.\nWhy Analyze Your Applications Clickstreams? Organizations today are in search of vetted solutions and architectural guidance to rapidly solve business challenges. Whether you prefer off-the-shelf deployments or customizable architectures, the AWS Solutions Library carries solutions built by AWS and AWS Partners for a broad range of industry and technology use cases.\nWhen I talk with mobile and web application developers or product owners, you often tell me that you want to use a clickstream analysis solution to understand your customers\u2019 behavior inside your application. Click stream analysis solutions help you to identify popular and frequently visited screens, analyze navigation patterns, identify bottlenecks and drop-off points, or perform A/B testing of functionalities such as the pay wall, but you face two challenges to adopt or build a click stream analysis solution.\nEither you use a third-party library and analytics solution that sends all your application and customer data to an external provider, which causes security and compliance risks and makes it more difficult to reference your existing business data to enrich the analysis, or you dedicate time and resources to build your own solution based on AWS services, such as Amazon Kinesis (for data ingestion), Amazon EMR (for processing), Amazon Redshift (for storage), and Amazon QuickSight (for visualization). Doing so ensures your application and customer data stay in the security perimeter of your AWS account, which is already approved and vetted by your information and security team. Often, building such a solution is an undifferentiated task that drives resources and budget away from developing the core business of your application.\nIntroducing Clickstream Analytics on AWS The new solution Clickstream Analytics on AWS provides you with a backend for data ingestion, processing, and visualization of click stream data. It\u2019s shipped as an AWS CloudFormation template that you can easily deploy into the AWS account of your choice.\nIn addition to the backend component, the solution provides you with purpose-built Java and Swift SDKs to integrate into your mobile applications (for both Android and iOS). The SDKs automatically collects data and provide developers with an easy-to-use API to collect application-specific data. They manage the low-level tasks of buffering the data locally, sending them to the backend, managing the retries in case of communication errors, and more.\nThe following diagram shows you the high-level architecture of the solution.\n\nThe solution comes with an easy-to-use console to configure your solution. For example, it allows you to choose between three AWS services to ingest the application clickstream data: Amazon Managed Streaming for Apache Kafka, Amazon Kinesis Data Streams, or Amazon Simple Storage Service (Amazon S3). You can create multiple data pipelines for multiple applications or teams, each using a different configuration. This allows you to adjust the backend to the application user base and requirements.\nYou can use plugins to transform the data during the processing phase. The solution comes with two plugins preinstalled: User-Agent enrichment and IP address enrichment to add additional data that\u2019s related to the User-Agent and the geolocation of the IP address used by the client applications.\nBy default, it provides a Amazon Redshift Serverless cluster to minimize the costs, but you can select a provisioned Amazon Redshift configuration to meet your performance and budget requirements.\nFinally, the solution provides you with a set of pre-assembled visualization dashboards to report on user acquisition, user activity, and user engagement. The dashboard consumes the data available in Amazon Redshift. You\u2019re free to develop other analytics and other dashboards using the tools and services of your choice.\nLet\u2019s See It in Action The best way to learn how to deploy and to configure Clickstream Analytics on AWS is to follow the tutorial steps provided by the Clickstream Analytics on AWS workshop.\nThe workshop goes into great detail about each step. Here are the main steps I did to deploy the solution:\n1. I create the control plane (the management console) of the solution using this CloudFormation template. The output of the template contains the URL to the management console. I later receive an email with a temporary password for the initial connection.\n2. On the Clickstream Analytics console, I create my first project and define various network parameters such as the VPC, subnets, and security groups. I also select the service to use for data ingestion and my choice of configuration for Amazon Redshift.\n\n\n3. When I enter all configuration data, the console creates the data plane for my application.\nAWS services and solutions are usually built around a control plane and one or multiple data planes. In the context of Clickstream Analytics, the control plane is the console that I use to define my data acquisition and analysis project. The data plane is the infrastructure to receive, analyze, and visualize my application data. Now that I define my project, the console generates and launches another CloudFormation template to create and manage the data plane.\n4. The Clickstream Analytics console generates a JSON configuration file to include into my application and it shares the Java or Swift code to include into my Android or iOS application. The console provides instructions to add the clickstream analysis as a dependency to my application. I also update my application code to insert the code suggested and start to deploy.\n\n5. After my customers start to use the mobile app, I access the Clickstream Analytics dashboard to visualize the data collected. \nThe Dashboards Clickstream Analytics dashboards are designed to provide a holistic view of the user lifecycle: the acquisition, the engagement, the activity, and the retention. In addition, it adds visibility into user devices and geographies. The solution automatically generates visualizations in these six categories: Acquisition, Engagement, Activity, Retention, Devices, and Navigation path. Here are a couple of examples.\nThe Acquisition dashboard reports the total number of users, the registered number of users (the ones that signed in), and the number of users by traffic source. It also computes the new users and registered users\u2019 trends.\n\nThe Engagement dashboard reports the user engagement level (the number of user sessions versus the time users spent on my application). Specifically, I have access to the number of engaged sessions (sessions that last more than 10 seconds or have at least two screen views), the engagement rate (the percentage of engaged sessions from the total number of sessions), and the average engagement time.\n\nThe Activity dashboard shows the event and actions taken by my customers in my application. It reports data, such as the number of events and number of views (or screens) shown, with the top events and views shown for a given amount of time.\n\nThe Retention tab shows user retention over time: the user stickiness for your daily, weekly, and monthly active users. It also shows the rate of returning users versus new users.\n\nThe Device tab shows data about your customer\u2019s devices: operating systems, versions, screen sizes, and language.\n\nAnd finally, the Path explorer dashboard shows your customers\u2019 navigation path into the screens of your applications.\n\nAs I mentioned earlier, all the data are available in Amazon Redshift, so you\u2019re free to build other analytics and dashboards.\nPricing and Availability The Clickstream Analytics solution is available free of charge. You pay for the AWS services provisioned for you, including Kinesis or Amazon Redshift. Cost estimates depend on the configuration that you select. For example, the size of the Kinesis and Amazon Redshift cluster you select for your data ingestion and analytics needs, or the volume of data your applications send to the pipeline both affect the monthly cost of the solution.\nTo learn how to get started with this solution, take the Clickstream Analytics workshop today and stop sharing your customer and application clickstream data with third-party solutions.\nSeb has been writing code since he first touched a Commodore 64 in the mid-eighties. He inspires builders to unlock the value of the AWS cloud, using his secret blend of passion, enthusiasm, customer advocacy, curiosity and creativity. His interests are software architecture, developer tools and mobile computing. If you want to sell him something, be sure it has an API. Follow him on Twitter @sebsto.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/aws-net-distributed-cache-provider-dynamodb/",
      "title": "  Introducing the AWS .NET Distributed Cache Provider for DynamoDB  ",
      "summary": "AWS releases .NET Distributed Cache Provider for DynamoDB, enabling DynamoDB as storage for ASP.NET Core's distributed cache to improve app performance and share data across servers.",
      "content": " Posted On:  Jul 7, 2023 \nWe are happy to announce the preview release of the AWS .NET Distributed Cache Provider for DynamoDB. This library enables Amazon DynamoDB to be used as the storage for ASP.NET Core\u2019s distributed cache framework.\u00a0\nA cache can improve the performance of an application; an external cache allows the data to be shared across application servers and helps to avoid cache misses when the application is restarted or redeployed. Customers can now use DynamoDB tables to cache their session state using ASP.NET Core\u2019s distributed cache framework.\u00a0\nGet started by installing the AWS.DistributedCacheProvider package from NuGet.org. It is open sourced, and we welcome community contributions! To learn more, go to our blog post, visit our GitHub page and our developer documentation.\u00a0\u00a0\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/dynamodb-local-version-2-0/",
      "title": "  Announcing DynamoDB local version 2.0 ",
      "summary": "DynamoDB local migrated to jakarta.* namespace, enabling Java devs to use it with Spring Boot 3, Spring Framework 6, and Micronaut 4 for building cloud-native apps locally without incurring costs.",
      "content": " Posted On:  Jul 5, 2023 \nToday, Amazon DynamoDB local, a local downloadable version of Amazon DynamoDB, has migrated to use the jakarta.* namespace. This latest version allows Java developers to use DynamoDB local to work with Spring Boot 3 and frameworks such as Spring Framework 6 and Micronaut Framework 4 to build modernized, simplified, and lightweight cloud native applications.\nYou can develop and test applications by running DynamoDB local in your local development environment without incurring any additional costs. DynamoDB local does not require an internet connection and it works with your existing DynamoDB API calls. DynamoDB local is free to download and available for macOS, Linux, and Windows. Get started with the latest version by downloading it from \u201cDeploying DynamoDB locally on your computer\u201d. To learn more, see Setting Up DynamoDB Local (Downloadable Version).\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/aws-lambda-detects-recursive-loops-lambda-functions/",
      "title": "  AWS Lambda now detects and stops recursive loops in Lambda functions ",
      "summary": "AWS Lambda now detects and stops recursive loops in Lambda functions, preventing unintended usage and costs, with automatic Dead-Letter Queue handling and AWS Health Dashboard notifications.",
      "content": " Posted On:  Jul 13, 2023 \nAWS Lambda can now detect and stop recursive loops in Lambda functions. Customers build event-driven applications using Lambda functions to process events from sources like Amazon SQS and Amazon SNS. However, in certain scenarios, due to resource misconfiguration or code defect, a processed event may be sent back to the same service or resource that invoked the Lambda function. This can cause an unintended recursive loop, and result in unintended usage and costs for customers. With this launch, Lambda will stop recursive invocations between Amazon SQS, AWS Lambda, and Amazon SNS after 16 recursive calls.\nWhen a function sends an event to Amazon SQS or Amazon SNS using a supported AWS SDK version or higher, Lambda tracks the number of times a function has been invoked based on that event. If a function is invoked by the same triggering event more than 16 times, Lambda will stop the next invocation and sends the event to a Dead-Letter Queue or on-failure destination, if configured. Customers will also receive an AWS Health Dashboard notification with troubleshooting steps.\nThis feature is turned on by default, and is available in the following AWS Regions: Asia Pacific (Hong Kong, Jakarta, Osaka, Mumbai, Seoul, Singapore, Sydney, Tokyo), Africa (Cape Town), Canada (Central), Europe (Frankfurt, Ireland, London, Milan, Paris, Stockholm), South America (Sao Paulo), US East (Ohio, N.Virginia), US West (Oregon, N.California). To turn off the feature for your AWS Account, please contact AWS Support. For further information, please refer to our documentation or the launch blog post.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-cloudfront-3072-bit-rsa-certificates/",
      "title": "  Amazon CloudFront announces support for 3072-bit RSA certificates ",
      "summary": "Amazon CloudFront now supports 3072-bit RSA certificates, allowing customers to enhance communication security between clients and CloudFront edge locations.",
      "content": " Posted On:  Jul 14, 2023 \nAmazon CloudFront announces support for 3072-bit RSA certificates. Customers can now associate their 3072-bit RSA certificates with CloudFront distributions to enhance communication security between clients and CloudFront edge locations.\nRSA is a encryption algorithm widely used in digital certificates to secure internet communications through digital signatures and data encryption. Prior to this update, CloudFront customers could only use RSA certificates with 1024-bit or 2048-bit strength, or an ECDSA P256 certificate. ECDSA P256 certificates provide greater security than 1024-bit or 2048-bit RSA certificates, yet they might not be supported by legacy clients and devices. With the introduction of 3072-bit RSA certificates, customers can now achieve the same security level in CloudFront previously exclusive to ECDSA P256 certificates.\nAmazon CloudFront's support for 3072-bit RSA certificates is now available for immediate use. To get started, associate a 3072-bit RSA certificate with your CloudFront distribution using console or APIs. There are no additional fees associated with this feature. For more information, please refer to the CloudFront Developer Guide. To learn more about CloudFront, visit the CloudFront Getting Started page.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/dynamodb-local-version-2-0/",
      "title": "  Announcing DynamoDB local version 2.0 ",
      "summary": "DynamoDB local now uses jakarta.* namespace, enabling Java devs to use it with Spring Boot 3, Spring Framework 6, and Micronaut 4 for building cloud-native apps locally without internet or costs.",
      "content": " Posted On:  Jul 5, 2023 \nToday, Amazon DynamoDB local, a local downloadable version of Amazon DynamoDB, has migrated to use the jakarta.* namespace. This latest version allows Java developers to use DynamoDB local to work with Spring Boot 3 and frameworks such as Spring Framework 6 and Micronaut Framework 4 to build modernized, simplified, and lightweight cloud native applications.\nYou can develop and test applications by running DynamoDB local in your local development environment without incurring any additional costs. DynamoDB local does not require an internet connection and it works with your existing DynamoDB API calls. DynamoDB local is free to download and available for macOS, Linux, and Windows. Get started with the latest version by downloading it from \u201cDeploying DynamoDB locally on your computer\u201d. To learn more, see Setting Up DynamoDB Local (Downloadable Version).\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/aws-dms-amazon-redshift-target/",
      "title": "  AWS DMS now supports Amazon Redshift Serverless as a target ",
      "summary": "AWS DMS now supports Amazon Redshift Serverless as a target endpoint, enabling secure data migration without infrastructure management.",
      "content": " Posted On:  Jul 14, 2023 \nAWS Database Migration Service (AWS DMS) now supports Amazon Redshift Serverless as a target endpoint. With this new support, you can securely migrate your data to Redshift Serverless, where it can be stored, queried, and analyzed without the need to provision or manage infrastructure.\nAWS DMS supports both full load and Change Data Capture (CDC) migration modes for Redshift Serverless. Full Load migration copies all of the data from the source database to Redshift Serverless. CDC copies only the data that has changed since the last migration.\nTo migrate your data to Redshift Serverless, you can use the AWS DMS console, AWS CLI, or AWS SDKs. To learn more, refer to using Amazon Redshift Serverless as a target for AWS DMS.\u00a0\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-s3-inventory-access-control-lists-metadata-inventory/",
      "title": "  Amazon S3 Inventory can include ACLs as object metadata in inventory reports ",
      "summary": "Amazon S3 Inventory now provides details on object ACLs, enabling easy review of access permissions before migrating to IAM-based bucket policies and disabling ACLs with S3 Object Ownership.",
      "content": " Posted On:  Jul 14, 2023 \nWith Amazon S3 Inventory, you can now easily review your access control lists (ACLs) on all of your objects to simplify review of access permissions. ACLs were the original way to manage object access when S3 launched in 2006. Now, when migrating to IAM-based bucket policies for access control, you can easily review all of the object ACLs in your buckets before enabling S3 Object Ownership.\nS3 Inventory provides a complete list of objects in a bucket and their corresponding metadata. The new Object ACLs fields include details about the object owner and the grantee along with their permission granted. You can activate reporting on object ACLs by editing existing S3 Inventory configuration in the AWS Management Console or API.\nBy enabling S3 Object Ownership, you can change how S3 performs access control for a bucket so that only IAM policies are used. S3 Object Ownership's \u2018Bucket owner enforced\u2019 setting disables ACLs for your bucket and the objects in it, and updates every object so that each object is owned by the bucket owner. We recommend that you carefully review your use of ACLs with inventory reports, migrate to IAM-based bucket policies, and then disable ACLs with S3 Object Ownership. For more information, see Controlling ownership of objects and disabling ACLs for your bucket.\nAmazon S3 Inventory support for Object ACL is generally available at no additional charge in all AWS Regions, excluding the AWS GovCloud (US) Regions and AWS China Regions. To learn more, please visit Amazon S3 Inventory and Amazon S3 pricing.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-location-service-device-updates-eventbridge/",
      "title": "  Amazon Location Service now supports publishing device position updates on EventBridge ",
      "summary": "Amazon Location Service now supports publishing tracked device position updates on Amazon EventBridge, enabling real-time location-based applications and long-term movement data analysis.",
      "content": " Posted On:  Jul 11, 2023 \nAmazon Location Service now supports publishing tracked device position updates on Amazon EventBridge, allowing customers to leverage position updates to deliver features tailored to the physical location of tracked devices. Developers can create applications that show devices as they move on a map, or store movement data in long-term storage to be used for purposes like asset movement insights, predictive analytics, or compliance.\u00a0\nAmazon Location Service enables customers to store and retrieve the current and historical location of tracked devices, and helps customers filter out position updates from devices that haven't moved to reduce cost and simplify application development. Now with Amazon EventBridge integration, developers can publish once and use EventBridge rules to further filter and route device-moved events to one or more downstream services. For example, a developer building a delivery application can update the location of a delivery vehicle in near real-time, if and when the vehicle has moved. Additionally, a customer looking to analyze patterns, trends, and behaviors related to the movement of people, vehicles, or assets over time can now send device-moved position updates to a long-term storage system, like an S3 bucket, for future processing or analytics.\nAmazon Location Service is available in the following AWS Regions: US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central), Europe (Frankfurt), Europe (Ireland), Europe (London), Europe (Stockholm), and South America (S\u00e3o Paulo).\nTo learn more, visit the Amazon Location Service Developer Guide.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-location-services-api-keys-maps-places-routes/",
      "title": "  Amazon Location Service now supports API Keys for Maps, Places, and Routes ",
      "summary": "Amazon Location Service now supports API keys for authentication, simplifying location-based application development and management of access to Maps, Places, and Routes resources.",
      "content": " Posted On:  Jul 10, 2023 \nAmazon Location Service now\u00a0supports API keys as an alternative for authenticating Maps, Places, and Routes resources, making it easier for developers to create authenticated Amazon Location resources. With API Keys, developers can easily create, manage, and expire access to Amazon Location resources, making it simpler for location based applications to interoperate with Amazon Location Service.\nIn addition to AWS Signature V4 and Amazon Cognito, developers adding location-based features to their applications can now use API keys to authenticate their interactions with Amazon Location resources. API keys can be used to track and control usage, manage billing and costs, and help secure access to Maps, Places, and Routes APIs.\u00a0 \nAmazon Location Service is a fully managed service that helps developers easily and securely add maps, points of interest, geocoding, routing, tracking, and geofencing to their applications without compromising on data quality, user privacy, or cost. With Amazon Location Service, you retain control of your location data, protecting your privacy and reducing enterprise security risks. \nAmazon Location Service is available in the following AWS Regions: US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central), Europe (Frankfurt), Europe (Ireland), Europe (London), Europe (Stockholm), and South America (S\u00e3o Paulo). \nWeb developers can use Amazon Location\u2019s authentication library to seamlessly authenticate their API calls and easily integration location-based features into their applications. To learn more, visit to the Amazon Location Service Developer Guide.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-aurora-postgresql-additional-versions/",
      "title": "  Amazon Aurora supports PostgreSQL 15.3, 14.8, 13.11, 12.15, and 11.20 versions ",
      "summary": "Amazon Aurora PostgreSQL-Compatible Edition now supports newer PostgreSQL versions with improvements and bug fixes, along with updates to Babelfish and AWS Database Migration Service.",
      "content": " Posted On:  Jul 13, 2023 \nFollowing the announcement of updates to the PostgreSQL database by the open source community, we have updated Amazon Aurora PostgreSQL-Compatible Edition to support PostgreSQL 15.3, 14.8, 13.11, 12.15, and 11.20. These releases contains product improvements and bug fixes made by the PostgreSQL community, along with Aurora-specific improvements. This release also contains new features and improvements for Babelfish for Aurora PostgreSQL version 3.2 and improved support for Amazon Web Services Database Migration Service version 3.5.1 Aurora PostgreSQL target endpoint Babelfish data types. Refer to the Aurora version policy to help you to decide how often to upgrade and how to plan your upgrade process. As a reminder, if you are running any version of Amazon Aurora PostgreSQL 11, you must upgrade to a newer major version by January 31, 2024. \nYou can initiate a minor version upgrade by modifying your DB cluster. Please review the Aurora documentation to learn more. This release is available in all AWS Regions except China regions, but including AWS GovCloud (US) Regions. For a full feature parity list, head to our feature parity page, and to see all regions that support Amazon Aurora head to our region page.\nAmazon Aurora is designed for unparalleled high performance and availability at global scale with full MySQL and PostgreSQL compatibility. It provides built-in security, continuous backups, serverless compute, up to 15 read replicas, automated multi-Region replication, and integrations with other AWS services. To get started with Amazon Aurora, take a look at our getting started page.\n"
    },
    {
      "link": "https://aws.amazon.com/new/?whats-new-content-all.sort-by=item.additionalFields.postDateTime&whats-new-content-all.sort-order=desc&awsf.whats-new-categories=*all&awsm.page-whats-new-content-all=1",
      "title": " What's New with AWS?",
      "summary": "AWS announces new capabilities for Parameter Store, CloudFormation, and EC2 Free Tier, as well as updates on Aurora Optimized Reads and Systems Manager Automation Runbooks Visual Designer.",
      "content": "AWS is constantly adding new capabilities so you can leverage the latest technologies to experiment and innovate more quickly. What's New posts show how we are doing just that, providing a brief overview of all AWS service, feature, and region expansion announcements as they are released. \nBrowse the page below to learn about our latest innovations. \nPutting your data to work with generative AI.\u00a0See the event agenda here\u00a0\u00bb\nParameter Store, a capability of AWS Systems Manager that provides secure storage for configuration data, now allows you to share advanced-tier parameters with other AWS accounts, enabling you to centrally manage your configuration data.  \nAWS CloudFormation launches a new feature that makes it easy to generate AWS CloudFormation templates and AWS CDK apps for existing AWS resources that are managed outside CloudFormation. \nStarting today, we are updating the AWS Free Tier for Amazon Elastic Compute Cloud, 12 month free, to include 750 hours of public IPv4 address usage per month. \nModel your world with myApplications \nHow Amazon Aurora Optimized Reads improves performance of gen AI apps \nAWS Systems Manager Automation Runbooks Visual Designer \nClick on the links below to continue exploring what AWS has to offer.\n"
    },
    {
      "link": "https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-documentdb-mongodb-index-improvements/",
      "title": "  Amazon DocumentDB (with MongoDB compatibility) launches index improvements ",
      "summary": "Amazon DocumentDB launches index improvements for faster index builds and ability to view index build status, available in all regions at no extra cost.",
      "content": " Posted On:  Jul 13, 2023 \nAmazon DocumentDB launches index improvements enabling faster index builds on collections and the ability to view index build statuses. Amazon DocumentDB index builds can now be sped by up to 14X when using parallel workers compared to using a single worker. The index creation process now uses two workers by default, and you can configure the number of workers on Amazon DocumentDB 4.0 and 5.0 instance-based clusters.\nTo get detailed insights into the index build speed and the current progress, you can now use existing APIs to retrieve indexing status metrics, such as the total number of keys to be indexed, and the number of keys already indexed at different points in time. The new index build status is now available on DocumentDB 5.0 instance-based clusters.\nThe new index improvements are in all regions where Amazon DocumentDB is available at no additional cost. To learn more, refer to the Managing Amazon DocumentDB indexes documentation.\n"
    }
  ]
}