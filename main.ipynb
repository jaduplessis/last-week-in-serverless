{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing articles/raw/Why I (Still) Like the Serverless Framework over the CDK _ DeBrie Advisory.pdf\n",
      "Processing articles/raw/Event-Driven Architectures vs. Event-Based Compute in Serverless Applications _ DeBrie Advisory.pdf\n",
      "Processing articles/raw/Key Takeaways from the DynamoDB Paper _ DeBrie Advisory.pdf\n",
      "Processing articles/raw/How you should think about DynamoDB costs _ DeBrie Advisory.pdf\n",
      "Processing articles/raw/GraphQL, DynamoDB, and Single-table Design _ DeBrie Advisory.pdf\n",
      "Processing page 3/3\r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.preprocessing.pdf_extract import recursive_extract_text\n",
    "\n",
    "recursive_extract_text('articles/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 5/5\r"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.pdf_extract import extract_text\n",
    "\n",
    "pdf_path = 'articles/raw/Building a Massively Scalable Serverless Chat Application with AWS AppSync _ by Sarah Hamilton _ Serverless Transformation _ Medium.pdf'\n",
    "output_path = 'Building a Massively Scalable Serverless Chat Application with AWS AppSync _ by Sarah Hamilton _ Serverless Transformation _ Medium.txt'\n",
    "\n",
    "extract_text(pdf_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing articles/parsed/Event-Driven Architectures vs. Event-Based Compute in Serverless Applications _ DeBrie Advisory.txt\n",
      "Processing articles/parsed/Key Takeaways from the DynamoDB Paper _ DeBrie Advisory.txt\n",
      "Processing articles/parsed/Why I (Still) Like the Serverless Framework over the CDK _ DeBrie Advisory.txt\n",
      "Processing articles/parsed/How you should think about DynamoDB costs _ DeBrie Advisory.txt\n",
      "Processing articles/parsed/GraphQL, DynamoDB, and Single-table Design _ DeBrie Advisory.txt\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.chunk import recursive_chunk_text\n",
    "from src.preprocessing.chunk import chunk_text\n",
    "import os\n",
    "\n",
    "# delete articles/chunks/chunks.txt\n",
    "if os.path.exists('articles/chunks/chunks.txt'):\n",
    "    os.remove('articles/chunks/chunks.txt')\n",
    "    \n",
    "recursive_chunk_text('articles/parsed/')\n",
    "# chunk_text('articles/parsed/Building a Massively Scalable Serverless Chat Application with AWS AppSync _ by Sarah Hamilton _ Serverless Transformation _ Medium.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutralising chunk: 29/29\r"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.neutralise import create_neutralised_dataset\n",
    "\n",
    "neutraliser = create_neutralised_dataset('articles/chunks/chunks.txt', 'articles/neutralised/training_data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64891\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "with open('articles/neutralised/neutralised_chunks.jsonl', 'r') as file:\n",
    "    chunks = file.read()\n",
    "\n",
    "print(num_tokens_from_string(chunks, \"cl100k_base\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Loading\n",
    "\n",
    "#### 1. Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 9 sections from the webpage.\n",
      "Extracted 7 sections from the webpage.\n",
      "Extracted 9 sections from the webpage.\n",
      "Extracted 10 sections from the webpage.\n",
      "Extracted 15 sections from the webpage.\n",
      "Extracted 12 sections from the webpage.\n",
      "Extracted 9 sections from the webpage.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.preprocessing.web_extract import extract_sections\n",
    "\n",
    "urls = [\n",
    "  \"https://www.alexdebrie.com/posts/event-driven-vs-event-based\",\n",
    "  \"https://www.alexdebrie.com/posts/dynamodb-costs\",\n",
    "  \"https://www.alexdebrie.com/posts/serverless-framework-vs-cdk\",\n",
    "  \"https://www.alexdebrie.com/posts/dynamodb-paper\",\n",
    "  \"https://www.alexdebrie.com/posts/dynamodb-eventual-consistency\",\n",
    "  \"https://www.alexdebrie.com/posts/dynamodb-graphql\",\n",
    "  \"https://www.alexdebrie.com/posts/dynamodb-partitions\"\n",
    "]\n",
    "\n",
    "extract_sections(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.chunking.chunks import Chunking \n",
    "\n",
    "chunking = Chunking(input_dir='articles/parsed/')\n",
    "chunks = chunking.chunk_corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Neutralising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic succeeded 2/188\n",
      "Anthropic succeeded 6/188\n",
      "Anthropic succeeded 7/188\n",
      "Anthropic succeeded 9/188\n",
      "Anthropic succeeded 11/188\n",
      "Anthropic succeeded 12/188\n",
      "Anthropic succeeded 14/188\n",
      "Anthropic succeeded 17/188\n",
      "Anthropic succeeded 19/188\n",
      "Anthropic succeeded 23/188\n",
      "Anthropic succeeded 25/188\n",
      "Anthropic succeeded 27/188\n",
      "Anthropic succeeded 31/188\n",
      "Anthropic succeeded 35/188\n",
      "Anthropic succeeded 38/188\n",
      "Anthropic succeeded 39/188\n",
      "Anthropic succeeded 40/188\n",
      "Anthropic succeeded 42/188\n",
      "Anthropic succeeded 43/188\n",
      "Anthropic succeeded 44/188\n",
      "Anthropic succeeded 45/188\n",
      "Anthropic succeeded 47/188\n",
      "Anthropic succeeded 53/188\n",
      "Anthropic succeeded 57/188\n",
      "Anthropic succeeded 58/188\n",
      "Anthropic succeeded 59/188\n",
      "Anthropic succeeded 61/188\n",
      "Anthropic succeeded 62/188\n",
      "Anthropic succeeded 63/188\n",
      "Anthropic succeeded 66/188\n",
      "Anthropic succeeded 67/188\n",
      "Anthropic succeeded 71/188\n",
      "Anthropic succeeded 72/188\n",
      "Anthropic succeeded 73/188\n",
      "Anthropic succeeded 77/188\n",
      "Anthropic succeeded 78/188\n",
      "Anthropic succeeded 79/188\n",
      "Anthropic succeeded 80/188\n",
      "Anthropic succeeded 83/188\n",
      "Anthropic succeeded 88/188\n",
      "Anthropic succeeded 89/188\n",
      "Anthropic succeeded 91/188\n",
      "Anthropic succeeded 92/188\n",
      "Anthropic succeeded 96/188\n",
      "Anthropic succeeded 97/188\n",
      "Anthropic succeeded 100/188\n",
      "Anthropic succeeded 102/188\n",
      "Anthropic succeeded 106/188\n",
      "Anthropic succeeded 107/188\n",
      "Anthropic succeeded 110/188\n",
      "Anthropic succeeded 115/188\n",
      "Anthropic succeeded 116/188\n",
      "Anthropic succeeded 117/188\n",
      "Anthropic succeeded 119/188\n",
      "Anthropic succeeded 124/188\n",
      "Anthropic succeeded 132/188\n",
      "Anthropic succeeded 133/188\n",
      "Anthropic succeeded 136/188\n",
      "Anthropic succeeded 137/188\n",
      "Anthropic succeeded 138/188\n",
      "Anthropic succeeded 143/188\n",
      "Anthropic succeeded 145/188\n",
      "Anthropic succeeded 149/188\n",
      "Anthropic succeeded 150/188\n",
      "Anthropic succeeded 153/188\n",
      "Anthropic succeeded 154/188\n",
      "Anthropic succeeded 155/188\n",
      "Anthropic succeeded 158/188\n",
      "Anthropic succeeded 160/188\n",
      "Anthropic succeeded 163/188\n",
      "Anthropic succeeded 164/188\n",
      "Anthropic succeeded 165/188\n",
      "Anthropic succeeded 169/188\n",
      "Anthropic succeeded 171/188\n",
      "Anthropic succeeded 172/188\n",
      "Anthropic succeeded 175/188\n",
      "Anthropic succeeded 176/188\n",
      "Anthropic succeeded 177/188\n",
      "Anthropic succeeded 179/188\n",
      "Anthropic succeeded 182/188\n",
      "Anthropic succeeded 184/188\n",
      "Anthropic succeeded 186/188\n",
      "Anthropic succeeded 187/188\n",
      "Anthropic succeeded 188/188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"- This post offers information on DynamoDB Partitions.\\n- DynamoDB powers high-traffic systems worldwide, including Amazon.com's shopping cart, real-time ad platform bidding, and low-latency gaming applications.\\n- DynamoDB's popularity is due to its fast, consistent performance regardless of scale.\\n- The consistent and predictable scaling properties of DynamoDB are due to basic computer science principles, not superior computing power.\",\n",
       " \"- This post will delve into DynamoDB partitions: what they are, why they matter, and how they should influence data modeling.\\n- Understanding DynamoDB partitions is crucial for comprehending DynamoDB's behavior and the rationale behind its API restrictions and single-table design principles.\\n- Initially, the DynamoDB API may seem unnecessarily restrictive, and single-table design principles may seem bizarre.\\n- However, once DynamoDB partitions are understood, the reasons for these aspects become clear.\\n- The post will cover the following sections:\\n  - (List of sections to be covered)\",\n",
       " \"- The section starts by discussing the basics of partitions in DynamoDB.\\n- To understand this, it's necessary to review some basic concepts about DynamoDB tables.\\n- DynamoDB is a schemaless database, so it doesn't verify the shape of items written into a table.\\n- However, a DynamoDB table isn't completely formless, as a primary key must be specified when creating the table.\\n- Each item written into the table must include this primary key, which uniquely identifies each item.\\n- There are two types of primary keys in DynamoDB: a simple primary key consisting of a single element known as the partition key. An example of a table with a simple primary key is provided.\",\n",
       " '- The table stores Orders for an e-commerce application.\\n- The primary key, on the left side of the table, is the \"OrderId\", marking it as a partition key.\\n- Each Order item in the table contains the \"OrderId\" property, which is unique for each item.\\n- The second type of primary key is a composite primary key.\\n- A composite primary key is made up of two elements: a partition key and a sort key.\\n- An example of a table with a composite primary key is shown.',\n",
       " \"- The table stores the same Orders as the previous table and also includes information about each Order's Customer.\\n- The primary key now consists of two parts: a partition key of CustomerId and a sort key of OrderId.\\n- Similar to the simple primary key, each item in this table will need to include both elements of the primary key.\\n- The combination of these two elements uniquely identifies each item in the table.\\n- Both primary key structures include a partition key element, which is crucial to DynamoDB's infinite scaling capabilities.\",\n",
       " \"- DynamoDB tables are distributed across multiple computers, not a single machine.\\n- Each computer holds a subset of the table's data called a 'partition'.\\n- Partitions enable essentially infinite scaling of a DynamoDB table.\\n- When writing an item to a table, the request is handled by a frontend service called the request router.\\n- The request router does not store data but routes the request to the correct partition.\\n- It parses the partition key from the item and hashes it.\\n- Based on the hashed partition key value, it determines which partition to write the item to.\",\n",
       " '- DynamoDB hashes the partition key before placing it on a partition to better distribute data across partitions.\\n- If sequential identifiers were used as partition keys without hashing, it could lead to hot partitions for certain access patterns.\\n- MongoDB follows a similar principle, as explained in a separate resource.\\n- In the provided image, the request router determined that the incoming item belonged to Partition 1 and sent the request there for handling.\\n- Reading items from DynamoDB follows a similar flow: the request router parses the partition key, routes the read request to the appropriate partition, reads the data, and returns it.',\n",
       " \"- DynamoDB partitions are approximately 10GB each; additional partitions are created as the table grows.\\n- A small table could have 2-3 partitions, but a large one could have thousands.\\n- The system scales effectively due to the request router's work in locating the correct node being an O(1) time complexity operation.\\n- Crucially, regardless of the number of partitions in DynamoDB's horizontal scaling, the request router's job remains a fast, consistent operation - reducing the size of the table from multiple terabytes to a more manageable 10GB.\\n\",\n",
       " '- Partitions in DynamoDB are transparent to users.\\n- Users do not need to know or specify partitions when reading or writing items.\\n- The request router handles partition selection and routing automatically.\\n- The specific partitions are unknown to users, as requests go through the request router.\\n- An undisclosed hash function is used to place items in partitions.\\n- This is different from some NoSQL databases where clients can directly access specific partitions.',\n",
       " \"- Using a shared request router leads to a slight performance reduction due to an additional network hop.\\n- Clients are not aware of partitions easing understanding for DynamoDB users.\\n- Clients do not need to request and maintain cluster metadata when interacting with DynamoDB, reducing the initial and ongoing data requirements.\\n- This unawareness enhances DynamoDB's ability to provide strong performance guarantees.\\n- Previously, understanding partitions and request distribution was important for high-scale tables.\\n- Provisioned throughput was distributed equally across all partitions, requiring capacity planning for the busiest partition.\",\n",
       " \"- In 2018, DynamoDB introduced adaptive capacity for partitions.\\n- Adaptive capacity allows provisioned throughput to be dynamically allocated to partitions based on demand, rather than being equally distributed.\\n- DynamoDB automatically manages capacity utilization by taking actions like:\\n  - Adding new partitions as data size grows\\n  - Splitting highly-used partitions into sub-partitions for consistent performance\\n- With adaptive capacity, you don't need to worry about balancing partitions manually.\\n- As long as you stay within the overall partition throughput limit, DynamoDB handles efficient capacity utilization.\",\n",
       " \"- Understanding DynamoDB partitions is helpful for building a theoretical mental model of how DynamoDB operates.\\n- The principles of single-table design can seem unconventional when coming from a normalized, relational database background.\\n- Grasping the importance of partitions in DynamoDB provides clarity on its data modeling approach.\\n- Once partitions are understood, the rest of DynamoDB's data modeling becomes more comprehensible.\",\n",
       " '- The importance of primary keys in DynamoDB is due to its partition structure.\\n- The request router uses the partition key to route requests to the correct partition.\\n- DynamoDB encourages including the partition key in requests.\\n- The DynamoDB API consists of three main action types.\\n- Except for the Scan operation, the partition key must be included in DynamoDB requests.\\n- DynamoDB is schemaless outside the primary key due to its partitioning structure.\\n- Access to items in DynamoDB is focused on the primary key, requiring its existence for proper indexing.',\n",
       " '- DynamoDB items have no enforced structure beyond the primary key.\\n- No foreign key relationships or typed columns like relational databases.\\n- Data is associated with the primary key, with no strict schema.\\n- Operates as a key-value store, with keys that can be simple (partition key) or compound (partition + sort key).\\n- Values are opaque blobs without enforced schemas.\\n- Despite being schemaless, data should still have defined schemas maintained in application code.\\n- Schema validation must be handled in the application layer, not relying on the database.',\n",
       " '- Other NoSQL databases use partitioning for horizontal scaling but do not always restrict the API to maintain this benefit.\\n- Careless use of scatter-gather queries to each partition can eliminate the advantages of horizontal scaling.\\n- Efficient queries in DynamoDB can be performed by the attributes in the primary key, not by other attributes in the table.\\n- The table design should reflect this.',\n",
       " '- DynamoDB\\'s partition model necessitates considering data distribution.\\n- A common mistake made by new DynamoDB users is grouping unrelated items in one item collection, which should be avoided.\\n- It is recommended to use a high-cardinality value for the partition key.\\n- For applications with users, \"userId\" is often a suitable partition key, as it distributes users across different item collections.\\n- Using boolean values or enums for partition keys is not advisable due to the low cardinality, which hinders data distribution.\\n- Thoughtful data distribution can help manage complex access patterns.',\n",
       " '- DynamoDB is often criticized for not handling full-text search or complex filtering patterns across the entire application dataset.\\n- For extensive full-text search requirements across the entire application, dedicated search infrastructure is recommended.\\n- However, search needs are often more limited in scope, confined to a smaller subset of the data.\\n- For example, you may want to enable full-text search only for users within a specific organization.\\n- In such cases, DynamoDB can work well by partitioning users by organization.\\n- When a search request comes in, you can retrieve all users for that organization and perform an in-memory search on the results before sending them to the client.\\n- If the search corpus is narrow (e.g., users within an organization), dedicated search infrastructure may not be necessary.',\n",
       " '- Amazon.com retail uses strategic searching in their e-commerce operations according to Rick Houlihan.\\n- When searching for a specific product, the backend refines the search area instead of going through the entire inventory.\\n- A similar strategy can be applied for geo-lookups â€“ breaking down the search into categories like countries, states, cities, geohashes, etc.\\n- After narrowing down the search space in DynamoDB, an in-memory filter can further refine the options.\\n- Understanding the role of partitions can improve efficiency by better segmenting data in DynamoDB.\\n',\n",
       " \"- DynamoDB's lack of support for joins is primarily due to its partitioning scheme.\\n- In a relational database, joins involve evaluating and combining portions of separate tables.\\n- This poses two challenges for DynamoDB:\\n    - The relevant data from the tables may reside on separate partitions, requiring additional network hops for separate reads.\\n    - A centralized node would be needed to combine the data from different partitions before returning to the client.\",\n",
       " \"- Clients may wish to join attributes not part of the primary key, conflicting with DynamoDB's emphasis on primary keys.\\n- Relational databases provide complex query planners that account for table structures, indices, and table content statistics.\\n- These challenges are not impossible to overcome in partitioned databases like DynamoDB.\\n- MongoDB provides the $lookup aggregation for outer join operations.\\n- Partitioned relational databases like Vitess support joins.\\n- However, these solutions increase complexity for both DynamoDB backend and users.\\n- Additional complexity also introduces performance variability, a disadvantage for DynamoDB.\",\n",
       " '- DynamoDB requires you to design without the use of joins.\\n- This may result in denormalization or pre-joining heterogenous items into the same partition.\\n- The approach can be selected according to specific requirements.\\n- Regardless of the chosen approach, there will be no uncertainty about performance implications as the application scales.',\n",
       " '- The post discussed DynamoDB partitions, their significance, and their effect on data modeling.\\n- The key lesson is the importance of understanding the philosophy of DynamoDB.\\n- Compared to relational databases, DynamoDB is not as complicated once key principles are learned, leading to an accurate mental model and easier usage.\\n- Readers are invited to share any questions or comments on the piece.',\n",
       " \"- DynamoDB's eventual consistency model is a common criticism, with concerns about its use in critical applications.\\n- Eventual consistency can introduce complications, but these can be managed in most cases.\\n- Even strongly consistent relational databases can have issues related to isolation or ORM choices.\\n- The benefits of accepting some eventual consistency in DynamoDB can be significant.\\n- The post aims to address concerns around eventual consistency in DynamoDB.\\n- The content will be covered in three sections.\",\n",
       " '- The discussion sets boundaries on the topic of eventual consistency.\\n- Confusion around eventual consistency is prevalent, partly due to the complex nature of database and distributed system consistency.\\n- An in-depth post was written covering three types of consistency, none of which was \\'eventual consistency\\'.\\n- Readers may find it helpful to read that post if the concepts in this background are unclear.\\n- \"Eventual consistency\", in high level terms, refers to a distributed systems model which ensures all accesses to a particular data item return the last updated value after a certain point, given no new updates are made.',\n",
       " '- The definition of eventual consistency outlines what it guarantees, but the key aspect is what it does not guarantee.\\n- It does not promise that any read of a data item will show the most recent updates to that item.\\n- This is the core drawback of eventual consistency - you could update an item, but a subsequent request may not reflect those updates.\\n- Three key aspects of eventual consistency will be discussed, with a focus on DynamoDB:',\n",
       " \"- Eventual consistency is principally related to data replication.\\n- In a single-node database, eventual consistency isn't an issue as writes and reads are centralized, allowing for a consistent view of data.\\n- When data is replicated across multiple nodes, managing eventual consistency becomes more complex.\\n- Upon a write to a node, an efficient way to communicate the write to other nodes is needed.\\n- If other nodes are allowed to read data, the PACELC tradeoff must be considered.\\n- Despite challenges, replication can be beneficial and there may be several reasons to replicate data across multiple locations.\",\n",
       " '- Redundancy: Adding replicas allows for redundancy in case the primary instance fails. Replicas serve as hot standby backups, without being actively read from (except during failure scenarios). This does not introduce eventual consistency implications on its own. This is the default mode for MongoDB replicas, where all read requests go to the primary node unless configured otherwise.\\n\\n- Increase throughput: Replication can be used to increase processing throughput. Write operations go to the primary node, but reads can be distributed across secondary nodes. This approach, similar to adding read replicas in relational databases, increases the available nodes for read traffic and allows handling more requests per second. However, it introduces the eventual consistency issues discussed in the post.',\n",
       " \"- Replication is used to bring data closer to global users, reducing network latency - this might introduce eventual consistency issues.\\n- Users may be global while the database could be centralized, causing slow requests due to network latency.\\n- Globally distributing data can significantly reduce client-side latency - this is the method Fly.io uses with global Postgres.\\n- The reasons for replication aren't mutually exclusive.\\n- In DynamoDB, all three reasons are relevant: additional replicas in different availability zones increase availability and throughput, and global tables put data closer to users.\",\n",
       " '- Eventual consistency is often associated with the CAP theorem, which is relevant but not the only consideration.\\n- The CAP theorem applies under specific conditions - failures and specifically network partitions between nodes.\\n- In normal operational times, there is a different trade-off to consider - latency versus consistency.\\n- This trade-off is part of the PACELC theorem.',\n",
       " '- Consider a three-node system with Node A as the primary node that handles the writes; all nodes can handle reads.\\n- Different choices exist for handling a write to the primary node.\\n- Option one is to wait for acknowledgement from all three nodes before confirming the write to the client, providing the highest consistency.\\n- This option increases latency due to the time taken to interact with remote nodes and waiting for the slowest response.\\n- This scenario can be compared to an RDBMS with synchronous replication to read replicas.',\n",
       " '- For writes, one option is to commit only to the primary node before acknowledging the client, with updates sent asynchronously to replica nodes.\\n- This approach has lower write latency for clients but higher risk of data inconsistencies on reads.\\n- It is similar to asynchronous read replicas in relational databases or MongoDB\\'s writeConcern: 1 setting.\\n- Another option is MongoDB\\'s writeConcern: \"majority\" setting, where the write must be committed to a majority of nodes before returning.\\n- This is a middle ground between the two extremes of consistency and latency.\\n- The different flavors of eventual consistency in DynamoDB will be discussed in terms of where they fall on this spectrum.',\n",
       " '- Eventual consistency is typically a challenge with reading data, not writing it.\\n- Most replicated databases use a primary-replica setup with a primary node responsible for all write operations and one or more replica nodes.\\n- Since the primary node updates replicas, there is no eventual consistency issue when writing data.\\n- This point is relevant when discussing strategies to handle eventual consistency.\\n- Some databases, like Amazon Aurora and DynamoDB Global Tables, allow for a multi-primary setup where any instance can handle write operations.\\n- A multi-primary system increases the complexity around eventual consistency and can potentially lead to subtle bugs. This will be discussed later.',\n",
       " \"- The discussion begins with eventual consistency on the main table in DynamoDB.\\n- DynamoDB tables have a primary key to uniquely identify each item which is heavily used for data access in the DynamoDB API.\\n- Secondary indexes are created when items need to be accessed in various ways. \\n- These indexes have a different primary key, with DynamoDB replicating data into the secondary index based on this new key structure. \\n- Only reads can be performed against the secondary index; all writes need to occur through the main table. \\n- The main table in DynamoDB allows either strongly consistent reads or eventually consistent reads. \\n- Explanation requires understanding of DynamoDB's underlying structure and the process of handling a write request.\",\n",
       " '- DynamoDB stores data across multiple partitions, with records assigned to a specific partition based on the partition key.\\n- A table record belongs only to one partition.\\n- Each partition comprises three nodes: a primary and two replicas.\\n- The primary node is responsible for taking care of writes while any node may handle reads.\\n- A write request in DynamoDB gets routed to the primary node using the partition key.\\n- The primary node applies the write if conditions are met and submits the write to both replica nodes.\\n- Once one of the replicas confirms a successful write, the primary node sends a successful response to the client.',\n",
       " '- DynamoDB ensures write durability by requiring a majority of nodes to commit the write.\\n- If the primary node fails after the write, the write is preserved on another node.\\n- Inconsistent reads can occur if a read request goes to a replica that has not yet received the latest write.\\n- When a write is accepted by the primary and one replica, a read request to the other lagging replica could return stale data without the latest updates.',\n",
       " '- DynamoDB chooses a middle ground on the latency-consistency spectrum.\\n- It neither ensures strong consistency by writing to all nodes nor minimizes latency by writing only to one node.\\n- The choice is partly motivated by the durability benefits of writing to multiple nodes.\\n- A stale read on the main table in DynamoDB is possible but unlikely.\\n- As DynamoDB randomly selects one of the three nodes for a read request and a write is committed to two of the three nodes before it is acknowledged, there\\'s a 66% chance of hitting a \"strongly consistent\" node.',\n",
       " '- The third node in the replication process is likely only milliseconds behind, based on concurrent sending of the write operation to both replicas.\\n- The same principles discussed for the main table also apply to local secondary indexes.\\n- Items in a local secondary index are housed on the same partition as those in the main table, leading to identical replication behavior.\\n- Detailed nuances of local secondary indexes warrant a separate discussion.',\n",
       " '- Eventual consistency in global secondary indexes.\\n- Unlike main tables and local secondary indexes, strongly consistent reads cannot be opted for global secondary indexes.\\n- The reason lies in the underlying infrastructure implementation.\\n- DynamoDB partitions data using the partition key.\\n- For a global secondary index, DynamoDB creates separate partition infrastructure.\\n- This is different from how other NoSQL databases like MongoDB handle additional indexes, which are implemented on existing shards holding the data.',\n",
       " '- DynamoDB places global secondary indexes on new partitions to maintain consistent and predictable query performance.\\n- DynamoDB aims for consistent response times, regardless of database size or concurrent query load.\\n- It achieves this through partitioning using the partition key, enabling quick access to the relevant ~10GB partition for requested data.\\n- However, global secondary indexes can have different partition keys than the main table.\\n- Without reindexing, a query on a global secondary index would require scanning every partition in the table, which is inefficient.\\n- This is the approach used by MongoDB, where queries not using the shard key must broadcast to all shards in the cluster.\\n- By reindexing data into new partitions for global secondary indexes, DynamoDB can maintain consistent and predictable query performance.',\n",
       " '- DynamoDB places global secondary indexes on separate partitions, which affects consistency.\\n- To achieve strong consistency from a node, the write must be committed before being acknowledged.\\n- This would require writing to the primary table, its replica, and two nodes for each global secondary index.\\n- With one global secondary index, the additional latency might be acceptable.\\n- However, DynamoDB allows up to 20 global secondary indexes by default, and more can be requested.\\n- Waiting for acknowledgments from 10+ nodes, each with its own capacity and potential failures, would significantly reduce the benefit of global secondary indexes.\\n- Hence, strong consistency across global secondary indexes is not feasible due to the introduced latency.',\n",
       " '- DynamoDB uses asynchronous replication for global secondary indexes.\\n- A write operation is committed to two of the three main table nodes and adds a record to an internal queue.\\n- The write is then acknowledged to the client.\\n- A background service processes the queue to update the secondary indexes.\\n- Global secondary indexes prioritize write latency over consistency.\\n- The difference in eventual consistent reads between the main table and the global secondary index is noted.\\n- The replication lag on the main table is typically minimal, while the lag for global secondary indexes is more noticeable.',\n",
       " '- Global secondary indexes in DynamoDB have separate capacity from the main table.\\n- If insufficient capacity is provisioned for the global secondary index, or if partition throughput limits are exceeded, replication lag can increase.\\n- This behavior differs from the main table, where exceeding provisioned capacity or partition throughput limits results in an error on the write request itself.',\n",
       " '- Eventual consistency in DynamoDB Global Tables.\\n- The original plan was to discuss the two types of eventual consistency in DynamoDB, highlighting the difference in replication lag between main tables and global secondary indexes.\\n- However, a third type of eventual consistency was identified - cross-region replication using DynamoDB Global Tables.\\n- DynamoDB Global Tables provide a managed way to replicate tables across multiple regions.\\n- Benefits of Global Tables include redundancy and resiliency against region failures, as well as bringing data closer to users by routing them to the nearest region.',\n",
       " '- DynamoDB Global Tables infrastructure is similar to that of global secondary indexes.\\n- When a write is received in one region, it is durably committed to two of the three nodes on the main table for the region, and the write is acknowledged to the client.\\n- The write is then asynchronously replicated to separate infrastructure, which is a separate table in a different region, unlike global secondary indexes where it is a different set of partitions in the same region.\\n- Like global secondary indexes, Global Tables optimize for latency on the latency-consistency spectrum.\\n- However, there are two additional considerations compared to global secondary indexes.',\n",
       " '- Global Tables in DynamoDB have longer replication latency compared to global secondary indexes due to the greater geographical distance between regions, leading to higher network latency. P99 latencies across regions can easily reach 100-200 milliseconds, impacting replication times.\\n\\n- Using Global Tables introduces write-based consistency issues into the application, whereas eventual consistency is primarily a read-based problem in DynamoDB.\\n\\n- With Global Tables, writes can occur in both regions, and these writes are replicated between regions, potentially causing various problems in the application, which will be discussed in the next section.',\n",
       " \"- Discussing strategies for dealing with DynamoDB's eventual consistency.\\n- Understanding the requirements of your application is key to selecting the right dealing strategies.\",\n",
       " '- Understanding the general latency characteristics of DynamoDB\\'s eventual consistency models is important.\\n- DynamoDB has three flavors of eventual consistency with varying replication lags.\\n- In practice, the effects of eventual consistency may rarely be seen in an application.\\n- A common consistency preference is \"read your writes,\" which ensures that a process\\'s read reflects its previous write.\\n- \"Read your writes\" does not guarantee consistency for reads from other processes, but provides sanity for a single workflow.',\n",
       " '- Operations can be directed to specific nodes if the database allows.\\n- In a load-balanced NoSQL database like DynamoDB, the server from which data is read cannot be chosen.\\n- The likelihood of achieving \"read your writes\" behavior with different types of eventual consistency can be estimated.\\n- Basic testing was done on \"read your writes\" behavior against a main table and a global secondary index.\\n- Complete results and methodology are available on a GitHub repository.\\n- Even with eventual consistency, a read against a main table will largely yield \\'read your write\\' consistency in most cases.',\n",
       " \"- Understanding general latency characteristics is useful but shouldn't form a basis for reliance.\\n- Anecdote from Ben Kehoe suggests unexpected issues may arise from assumptions about maximum time for eventual consistency in DynamoDB.\\n- Simple tests performed don't guarantee similar results in all situations.\\n- It's more likely for main table results to be more stable than global secondary index results, owing to minimum node requirements for writes.\\n- A recent study on DynamoDB reveals the use of log-only replicas to maintain low write latency and high durability, especially during partition failures.\\n- However, global secondary indexes present more potential failure modes, like underprovisioned throughput or exceeding partition throughput limits.\",\n",
       " '- Write conditions in DynamoDB can ensure consistency.\\n- Eventual consistency largely affects read-operations, while all writes are done against the latest version of the item.\\n- A ConditionExpression can be included when writing, which needs to be true for the write to proceed.\\n- Potential issues with eventual consistency can arise in scenarios like bank transactions where actions can be based on potentially out-dated information.',\n",
       " \"- The use of ConditionExpressions can prevent certain problems.\\n- A ConditionExpression could ensure the account balance exceeds the transaction amount to proceed.\\n- Multiple operations with their own ConditionExpressions can be combined into a DynamoDB transaction for ACID compliance across multiple accounts.\\n- Even relational databases may not avoid these issues.\\n- Todd Warszawski and Peter Bailis discuss how lower isolation levels can lead to concurrency bugs during transactions.\\n- Peter Bailis and colleagues also illustrate how ORMs can subtly fail in implementing concurrency-control mechanisms.\\n- It's crucial to understand one's infrastructure and dependencies to avoid such issues.\",\n",
       " '- Write conditions in DynamoDB can help prevent data from being saved in inconsistent or invalid states.\\n- They cannot solve all problems related to eventual consistency.\\n- Approaches such as pessimistic concurrency (locking), optimistic concurrency (version checking), or asserting data constraints during write operations could be used.',\n",
       " '- Avoid multi-region writes with DynamoDB Global Tables when possible.\\n- Eventual consistency is primarily a read issue, but can become a write issue with Global Tables across multiple regions.\\n- Writing to multiple regions has two potential issues:\\n  - Potential loss of nearly simultaneous writes updating different attributes.\\n  - Possibility of overwrites if the same attribute is updated concurrently across regions.\\n- DynamoDB tries to reconcile concurrent writes, but updates can be lost in specific situations.',\n",
       " '- Write conditions that are true in one region may not be true in another region due to unrepllicated data. This can cause issues when using condition expressions and DynamoDB Transactions across multiple regions.\\n- When using DynamoDB Global Tables, it is recommended to write an item in only one region.\\n- This may require selecting a dedicated \"write\" region and directing all writes there, regardless of higher latency.\\n- For certain use cases, items may naturally align with specific regions, thereby ensuring each write occurs in only one region.',\n",
       " \"- The aim of the post was to alleviate fears regarding eventual consistency.\\n- The content covered background information on eventual consistency.\\n- The three types of eventual consistency in DynamoDB were discussed.\\n- This discussion helped clarify the differences among the three types and developed an understanding of DynamoDB's infrastructure.\\n- Strategies for managing eventual consistency in DynamoDB were presented.\\n- The author encouraged readers to reach out with any questions or corrections.\",\n",
       " \"- The topic is about the compatibility between GraphQL and single-table design in DynamoDB.\\n- The author has previously engaged in discussions about data modeling with DynamoDB.\\n- The discussion about the compatibility of GraphQL and single-table design in DynamoDB was triggered by a tweet.\\n- The subsequent tweet suggested an update about a previous post concerning single-table design where GraphQL's compatibility was questionable.\\n- The decision to use single-table design with GraphQL depends on the reasons for using GraphQL.\\n- The limitations of discussing nuanced topics like this on Twitter are recognized.\",\n",
       " \"- Single-table design can be used with GraphQL, and some highly skilled developers are doing so.\\n- However, opting out of single-table design is not wrong, depending on specific needs and preferences.\\n- Amplify made the correct choice in defaulting to a table-per-model approach.\\n- For practical tips on using single-table design with AppSync + GraphQL, check out Adam Elmore's video on his setup.\\n- The video also covers conceptual points on when to use (or not use) single-table design principles.\\n- The post will provide full details on the following topics:\",\n",
       " '- Potential benefits of adopting GraphQL:\\n\\n- Ability to fetch only the required data, reducing over-fetching and under-fetching\\n- Single endpoint for all data needs, simplifying client-side code\\n- Strongly typed schema, improving development experience and enabling better tooling\\n- Aggregating data from multiple sources in a single request\\n- Evolving the API without versioning, enabling gradual product iteration\\n- Shifting complexity from client to server, simplifying client-side code\\n\\n- The specific reasons for adopting GraphQL may vary based on context, team needs, and product requirements.\\n- Some potential benefits, like network bandwidth savings, are not directly relevant to the single-table vs. multi-table debate.',\n",
       " '- API type safety is a key advantage of GraphQL.\\n- Single-page applications request data from a backend server to display information.\\n- In a shopping application, this could be product lists or shopping cart contents.\\n- On GitHub, it could be repositories or issues for a repository.\\n- REST APIs often have a data schema in theory, but in practice the data shape can be unreliable.\\n- Frontend developers need to implement defensive coding to ensure expected data properties are present.',\n",
       " '- GraphQL addresses data inconsistency issues by using a defined, typed schema provided by the backend.\\n- Most GraphQL implementations verify that backend responses adhere to this schema prior to frontend delivery.\\n- As a frontend client using a GraphQL-based service, this provides more confidence in the returned data.',\n",
       " '- Reducing network requests from the frontend is another reason for using GraphQL.\\n- In a single-page application, rendering a shopping cart page with a REST backend may require multiple network calls:\\n    - To fetch the cart items\\n    - To fetch product details for each item\\n    - To fetch any additional data needed',\n",
       " \"- The N + 1 problem is commonly associated with ORMs making suboptimal database queries, adding load.\\n- However, single-page applications with RESTful backends also face this issue on the frontend.\\n- The frontend needs to make multiple requests to render a page, often sequentially.\\n- E.g., fetching shopping cart contents first, then making N calls to fetch item details.\\n- This waterfall of sequential requests can make the application seem sluggish.\\n- GraphQL reduces this problem by allowing a single request to retrieve a graph of data.\\n- In one GraphQL query, both the shopping cart and all items can be fetched.\\n- This eliminates the need for subsequent requests, allowing the entire page to render immediately.\\n- GraphQL doesn't necessarily eliminate the N + 1 problem entirely, potentially moving it to the backend.\\n- GraphQL eliminates the N + 1 problem for the frontend application and developer.\",\n",
       " '- Flexible querying patterns in GraphQL\\n- GraphQL eliminates the N+1 problem from the frontend\\n- Backends for Frontends (BFF) architecture pattern reduces calls from frontend to backend\\n- BFF implements backend APIs specifically for frontends\\n- Example: endpoint for rendering shopping cart assembles cart contents and item details before responding\\n- Fully implemented BFF pattern resembles GraphQL, providing full data graph in single request\\n- Main difference between BFFs and GraphQL is flexibility',\n",
       " '- Backend for Frontends (BFFs) are tailored for specific frontend experiences; as such, altering the frontend requires backend changes, potentially requiring additional teams and managing backward compatibility.\\n- BFFs might evolve slower due to these factors.\\n- GraphQL, on the other hand, provides more flexibility.\\n- With GraphQL, you define a schema representing your data structure.\\n- Clients can then request any data fitting the schema, and the GraphQL backend coordinates its retrieval. This means frontend changes do not usually necessitate backend modifications; altering the GraphQL query is usually sufficient.\\n- Normally, GraphQL backends use specific, isolated resolvers for data retrieval, which can lead to N + 1 queries.\\n- Example of this given is a shopping cart query.\\n',\n",
       " '- The cart resolver manages details about the overall shopping cart.\\n- A separate resolver handles details about the items.\\n- On the backend, these resolvers work sequentially.\\n- The items resolver is called to fetch the items after the cart resolver finishes.',\n",
       " '- GraphQL provides significant benefits for frontend developers:\\n  - No need to be defensive about data received from the API\\n  - Avoid waterfall of network requests and handling partial errors\\n  - Ability to iterate on frontend without backend assistance\\n  - Easy reshaping of responses by modifying GraphQL query input\\n- Resolvers are implemented generically, decoupled from originating query',\n",
       " '- GraphQL adoption is more common in teams where frontend developers have more influence.\\n- This is not inherently wrong, as engineering teams often have biases towards certain types of developers and optimize accordingly.\\n- REST APIs tend to be favored by backend developers as they simplify data requests.\\n- REST APIs push complexity to clients (often frontend developers) by limiting the data in individual requests.',\n",
       " '- While not going into depth on DynamoDB, some high-level features need to be discussed.\\n- DynamoDB is a NoSQL database from AWS, designed for extreme predictability and performance consistency.\\n- DynamoDB aims to offer the same performance for a query across all data sizes, from 1 MB to 1 PB.\\n- DynamoDB limits usage by enforcing the use of a primary key for most access patterns.\\n- The primary key includes a hash key and a B-tree for quick, consistent performance when operating on a single item or a range of items.\\n- Due to this primary key structure, response times on individual items are typically in the single-digit milliseconds, irrespective of database size.',\n",
       " '- DynamoDB lacks some features found in other databases.\\n- JOIN operations are not possible in DynamoDB.\\n- Similarly, aggregations across multiple records are not feasible.\\n- Both of these operations can have unpredictable performance on an unbounded number of items.\\n- DynamoDB does not support potentially problematic queries and lacks features that could facilitate them.',\n",
       " '- DynamoDB has strict limitations in certain areas, such as a 400KB maximum item size to encourage smaller, more focused items.\\n- It also has a limit on concurrent reads and writes to a data subset - exceeding this limit results in request throttling.\\n- These limits are enforced to uphold consistency and predictability, converting performance into either success or failure rather than degrading under load.\\n- Due to these design choices for ensuring consistency, data must be modeled differently in DynamoDB.\\n- Key consideration must be given to the actual usage of the data.',\n",
       " '- In relational databases, data is designed in an abstract, normalized manner, and queries/indexes are implemented for performance.\\n- In DynamoDB, access patterns are considered first, and the table is designed accordingly to handle those patterns.\\n- Data in DynamoDB is unlikely to be normalized as in relational databases; it is tailored for specific requirements.\\n- For the GitHub repository example, fetching a repository and its 10 most recent issues from separate tables in DynamoDB would require at least two requests, as DynamoDB lacks joins.\\n- Depending on the primary key modeling, it could involve two sequential requests, increasing latency.\\n- This relates to the N+1 problem (or the 1+1 problem in this case).',\n",
       " '- Single-table design can be employed when you need to access related data in a specific pattern, such as a repository and its ten most recent issues.\\n- The related items can be stored together in a single DynamoDB table.\\n- A Query operation can then fetch both the repository and issue items in a single request.\\n- This approach transforms an N+1 query into a single query, avoiding excessive CPU usage that might occur with SQL join operations.\\n- For more details on single-table design for one-to-many relationships, refer to the post on modeling one-to-many relationships in DynamoDB.',\n",
       " '- Single-table design in DynamoDB refers to using the Query operation to retrieve multiple, heterogeneous items in a single request.\\n- It is possible to use a single table without retrieving heterogeneous items, but in that case, the difference between one table or multiple tables is less pronounced.\\n- The author often uses a single table without retrieving heterogeneous items.',\n",
       " \"- Discussing the combination of GraphQL and single-table design benefits.\\n- The suitability of this combination depends on the value placed on GraphQL's flexibility.\\n- DynamoDB designs for known access patterns, while GraphQL allows for quick iteration on access patterns.\\n- Single-table design merges multiple entities in one table for nested, related data access in a single pattern.\\n- Earlier discussion referenced GraphQL implementations using isolated resolvers for fetching specific data.\\n- In the case of a complex query with nested data, GraphQL will pass it to sequential resolvers, each focused on their own data.\",\n",
       " \"- The N + 1 problem, earlier discussed, remains albeit shifted from the frontend to the backend.\\n- This shift can enhance performance due to the backend's closer proximity to the database, making multiple queries faster.\\n- The N + 1 problem can be reduced, if desired, by writing broader, more complex GraphQL resolvers.\\n- A top-level resolver can check other parts of the query to see if nested data has been requested. \\n- If so, the resolver can fetch the nested data during its database request.\\n- In a relational database, this probably requires a JOIN in your query.\\n- In DynamoDB, this means the application of single-table design principles to fetch multiple heterogeneous items in a single request.\",\n",
       " '- Using lookaheads in GraphQL resolvers is viewed as controversial and sometimes considered an anti-pattern.\\n- Marc-Andre Giroux, known for implementing large, public GraphQL APIs and writing \"Production-Ready GraphQL,\" advises against lookahead queries.\\n- He recommends keeping resolvers focused and narrow.\\n- Following this advice tends to eliminate most benefits of single-table design in DynamoDB.\\n- Data access and design should still be considered, but with a narrower scope.\\n- One should consider how specific entities will be accessed, not collections of diverse entities.',\n",
       " \"- Deciding on single-table design with GraphQL depends on trade-offs and application requirements.\\n- The majority of the GraphQL community aligns with Marc-Andre Giroux's advice:\\n  - Use isolated, focused resolvers that fetch specific data pieces.\\n  - Benefit: GraphQL query flexibility.\\n  - Downside: Reduced performance.\\n- Giroux's advice may stem from his experience designing public-facing GraphQL APIs for GitHub and Shopify.\\n- Public APIs require significantly more flexibility than internal applications.\",\n",
       " '- Implementing lookahead functionality with full query flexibility in the GitHub GraphQL API would result in significant resolver complexity.\\n- The Repository object in the GitHub GraphQL schema is immensely complex.\\n- Counting all the different relations on the Repository object becomes overwhelming, with 11 relations identified before reaching discussionCategories.\\n- Handling the variety of potential nested queries against a Repository object would be near impossible.\\n- Even for smaller GraphQL APIs, managing all the permutations of data can introduce a lot of complexity.',\n",
       " '- The issue being discussed is not specific to DynamoDB and GraphQL, but rather with GraphQL itself.\\n- The advice provided is applicable to all GraphQL APIs, regardless of the underlying database.\\n- Most GraphQL APIs make multiple, sequential requests to the database to handle nested queries, whether using DynamoDB, Postgres, or Neo4J.\\n- Some advocate for single-table design principles with GraphQL, as seen from Adam Elmore and Rich Buggy.\\n- In these cases, the GraphQL usage is likely from controlled clients with known access patterns.\\n- They leverage GraphQL for API type safety and fewer frontend network requests, but are less concerned about infinitely flexible APIs.',\n",
       " \"- The tweet mentioned a QCon talk about BBC's usage of GraphQL.\\n- The talk noted that GraphQL queries need to be registered in advance before going live.\\n- This suggests predefined, known access patterns rather than fully flexible queries.\\n- It's unknown if BBC optimizes their resolvers along with this approach.\\n- This can provide some GraphQL benefits without fully embracing flexible queries.\\n- Teams must weigh system performance versus flexibility based on context, team, and product needs.\",\n",
       " '- The discussion is whether the Amplify team implemented their solution correctly.\\n- Both single-table and multi-table models are considered acceptable choices with GraphQL.\\n- AWS Amplify, a toolkit on top of AWS AppSync, turns a GraphQL schema into a fully managed GraphQL server with databases and resolver code.\\n- Amplify creates a separate DynamoDB table for each object marked with an @model directive in the GraphQL schema.\\n- The question is whether the default setting towards multi-table was justified, and the belief is that it was, for two reasons.',\n",
       " \"- The GraphQL community generally favors isolated, focused resolvers, despite some examples of lookaheads enhancing performance.\\n- Single-table design in Amplify offers significant capabilities, aligning with its aim for a developer-friendly experience.\\n- Many Amplify users are not creating large-scale services and are mainly focused on developing MVPs and rapidly iterating.\\n- Optimizations can be considered later in the project's life cycle, not at the start.\",\n",
       " \"- Single-table design can be used with AppSync, as demonstrated by Rick's example of a single-table resolver and Rich Buggy's post on the topic.\\n- There is an asymmetry where someone familiar with single-table design can opt out of the defaults, but a newcomer to DynamoDB may be confused by single-table design and unaware of the multi-table option.\\n- While learning single-table design or new concepts can be challenging, multi-table is a valid option for AppSync, making it a defensible choice as the default.\\n- A potential issue with Amplify is that it can make it too easy to accidentally implement inefficient DynamoDB Scan operations, which can cause problems when an application goes into production.\",\n",
       " \"- The post emphasizes that both single-table and multi-table design are valid approaches with GraphQL and DynamoDB.\\n- GraphQL itself has its pros and cons and it's crucial to select what matters most for the implementation.\\n- Feedback, whether in form of questions, comments, or critique, can be shared through Twitter, comments on the post, or direct email.\",\n",
       " '- The topic is about understanding DynamoDB costs.\\n- A concern was raised regarding high DynamoDB costs for a specific use case, leading to a consideration for increasing application and architectural complexity.\\n- A common response to such queries is to ask if cost calculations have been done.\\n- One of the advantages of DynamoDB is the ease of calculating potential costs.\\n- This process can be used for understanding the rough cost for an application or for choosing among solutions for a specific access pattern.\\n- DynamoDB costs should be considered during model design to understand feasibility and evaluate tradeoffs.',\n",
       " \"- The post discusses the author's method for understanding DynamoDB costs.\\n- Begins with an overview of DynamoDB pricing (readers can skip this if already familiar).\\n- Includes examples of using this knowledge for decision-making regarding DynamoDB costs.\\n- The post focusses more on approach and incorporating cost modeling in the design process than being a comprehensive source on DynamoDB costs.\\n- Mentions further tips on DynamoDB costs are available in Khawaja Sham's thread discussing patterns for cost-saving with DynamoDB.\\n- Cites a recent Reddit thread on DynamoDB costs as another source of information, noting a mix of both correct and incorrect content.\",\n",
       " \"- An overview of how DynamoDB pricing works is provided.\\n- Pricing can be complex with various modes (on-demand vs provisioned) and table storage classes (Standard or Standard-IA).\\n- However, the primary aspects of DynamoDB's pricing are the core dimensions of charge.\\n- DynamoDB charges for three things.\\n- The first is read consumption measured in 4KB increments for every read operation. This represents the quantity of data read from the table or index.\\n- In the post, 'RCU' (read capacity unit) will denote each increment, although it's sometimes called 'read request unit'.\\n- For instance, a single 10KB item read in DynamoDB would consume 3 RCUs (10 / 4 rounded up to 3).\",\n",
       " '- Write consumption is measured in 1KB increments, rounded up for each write operation.\\n- Each 1KB increment consumed during a write is called a Write Capacity Unit (WCU).\\n- If writing a 7.5KB item, 8 WCUs will be consumed (7.5KB rounded up to 8KB).\\n- Storage is measured in GB-months, based on data stored in the table or index multiplied by the number of hours in the month.\\n- Read and write consumption are usually the dominant cost factors, with storage being less significant.\\n- The billing model is predictable, eliminating concerns about concurrent operations, instance types, or storage volumes.\\n- The focus is on the amount of data read, written, and stored.',\n",
       " '- DynamoDB\\'s pricing model allows for rough cost estimation during data modeling phase.\\n- The \"Doin\\' the Math\" section will cover cost calculation details.\\n- The cost calculation involves straightforward math that can be done using a spreadsheet.\\n- RCUs (Read Capacity Units) and WCUs (Write Capacity Units) are charged in increments, even if the full increment is not utilized.\\n  - Reading 1.5KB of data incurs a charge for 1 RCU.\\n  - Writing 0.5KB of data incurs a charge for 1 WCU.\\n- For read requests, there is an option for strongly consistent or eventually consistent reads.\\n- Eventually consistent read requests consume half the RCUs compared to strongly consistent reads.\\n- The recommendation is to almost always use eventually consistent read requests, unless there is a strong reason not to.',\n",
       " '- A DynamoDB Query allows multiple items to be read in a single request.\\n- In a Query operation, items are combined before division by 4KB to determine Read Capacity Unit (RCU) usage.\\n- Reading 100 items each with 100 bytes via a Query will costs 3 RCUs due to round up of 2.5 (calculation: 10KB/4)\\n- Reading each of the 100 items separately would cost 100 RCUs.\\n- Charges for reads are based on the data actually read, not the data returned to the client.\\n- Therefore, DynamoDB Filter Expressions may not help in the expected way.\\n- Key attributes should be used to filter results as much as possible.',\n",
       " '- Write operations in DynamoDB are charged based on the larger item size, before or after the operation.\\n- For example, incrementing a counter on a 9.5KB item will incur a charge for 10 WCUs.\\n- Deleting a 9.5KB item will also incur a charge for 10 WCUs.\\n- Failed condition expressions still incur WCU charges based on the matching item size (minimum 1 WCU).\\n- Using DynamoDB Transactions doubles the WCU usage due to the two-phase commit process under the hood.',\n",
       " '- Choosing between provisioned and on-demand billing modes for DynamoDB.\\n- Examples will use calculations based on DynamoDB On-Demand billing mode.\\n- On-Demand has fixed prices for reads and writes, making cost calculation easier.\\n- No need to consider utilization over time or handling traffic bursts, simplifying the math.\\n- Provides a directionally accurate estimate of potential costs.\\n- However, billing mode choice should still be considered after initial calculations.\\n- Fine-tuning can be done by comparing provisioned and on-demand pricing.',\n",
       " '- When on-demand usage is low, maintaining that should be considered for less operational complexity and avoiding throttling.\\n- On-demand solution is suitable if the estimated cost is affordable.\\n- However, if costs are high, exploring provisioned capacity could be an effective cost optimization.\\n- On-demand billing can be nearly seven times more costly than fully-utilized provisioned capacity.\\n- But achieving full utilization may not be likely, hence saving 85% might not be achievable.\\n- Yet, reaching 50% utilization could lead to 42% savings on the cost, which could be significant for high-traffic DynamoDB workloads.',\n",
       " \"- The responsibility for scaling to match traffic patterns falls on the user.\\n- If traffic increases progressively, the user must accommodate for the new peaks.\\n- In case of sudden traffic surges, it is the user's job to scale accordingly.\\n- Unexpected traffic spikes must also be managed by the user.\\n- Failure to adequately scale in response could result in throttling and subsequently, dissatisfied users.\",\n",
       " '- The discussion focuses on tracking view counts for a webpage using DynamoDB and its associated costs.\\n- This topic was initiated by an email inquiry about the costs of tracking page visits in DynamoDB, where each visit increases a counter for that page.\\n- There were concerns about potential high costs, leading to a proposal of batching page views in Redis temporarily and periodically updating DynamoDB.\\n- The addition of Redis would increase cost and complexity, such as managing multiple data sources and synchronizing data back to DynamoDB.\\n- A recommended approach to mitigate this fear is to \"do the math\" â€“ calculate item sizes, estimate traffic, and understand the costs using those variables. A spreadsheet could be helpful for this task.',\n",
       " \"- If the page record size is small (under 1KB), each write will consume 1 Write Capacity Unit (WCU).\\n- With on-demand billing at $1.25 per million WCUs, the estimated DynamoDB write cost would be $1.25 per million page views.\\n- The pricing may or may not be acceptable, but it provides an understanding of the costs involved.\\n- For the given scenario, it helped realize that optimizing DynamoDB costs was not a priority at that moment.\\n- To determine item size, generate a sample record and use Zac Charles' DynamoDB Item Size Calculator, which is a useful tool.\",\n",
       " '- An enhanced version of this concept, \"vertical sharding,\" was discussed at AWS re:Invent 2022.\\n- Vertical sharding involves splitting a single entity into multiple records to reduce costs, applying single-table design principles.\\n- In this example, if the underlying record was 10KB instead of 1KB, each million page views would cost $12.50, even though only incrementing a counter.\\n- To optimize costs, the record could be separated into two items: the page view counter and the actual item data.\\n- Both items can be placed in the same item collection for single Query retrieval when needed.\\n- Now, a page view increment only acts on the small, focused item, saving 90% of write costs in this example.',\n",
       " '- The post explores the need for the secondary index in DynamoDB with a more complex example.\\n- Each write to DynamoDB is charged at a rate of 1 WCU per KB of data.\\n- This charge applies to all targets of the write, including the base table and any secondary indexes.\\n- Having multiple secondary indexes on a DynamoDB table can increase write costs substantially.\\n- For instance, in a scenario of tracking per-page views with 5 secondary indexes, the cost per view could increase from 1 WCU to 6 WCUs.',\n",
       " \"- There are multiple methods to reduce costs for essential GSIs, but sometimes it's beneficial to avoid a GSI entirely.\\n- Consider an example to understand this better.\\n- In DynamoDB, it is usually recommended to filter and sort records using key attributes. However, occasionally, it might be better to read more data than necessary.\\n- Imagine a scenario where a SaaS service is being sold to small teams. It's highly unlikely for a team to include more than 100 people, with most teams comprising fewer than 10 users.\",\n",
       " '- Each User record within a team is small, around 1KB of data.\\n- There are a few access patterns for Users:\\n  - Point lookup to fetch a User by username\\n  - Range query to fetch all Users with a given role\\n  - Range query to fetch all Users on the team ordered by date added\\n- You could create a Global Secondary Index (GSI) for each secondary access pattern.\\n- Alternatively, you could fetch all Users on the team and filter them in the application code.\\n- In the worst case, fetching 100 Users (100KB) would require 12.5 Read Capacity Units (RCUs) for eventually consistent reads.\\n- In most cases, it would be much less than 1 RCU.\\n- RCUs are 4 times bigger than Write Capacity Units (WCUs) (4KB vs. 1KB).\\n- RCUs are also 5 times cheaper ($0.25 per million vs. $1.25 per million for WCUs).',\n",
       " '- Preliminary calculations suggest that for long-tail access patterns, it could be more beneficial to over-read data than to use a GSI, particularly when the overall result set is small.\\n- The focus should remain on using key attributes as much as possible. However, there are occasions where this approach can be relaxed to reduce write costs.',\n",
       " '- Complex filtering in DynamoDB involves filtering a dataset by multiple potential variables, all of which can be optional.\\n- DynamoDB prefers known access patterns, including the attributes used for filtering, making dynamic filtering challenging.\\n- For complex filtering requirements, performing calculations can help assess feasibility.\\n- Example: A CRM system tracking customers and their orders, allowing users to filter customers by name, email, city, industry segment, and date added to the system, with each attribute being optional in the filter.',\n",
       " '- Customer records can be large, around 4KB, making it impractical to add all access pattern permutations as GSIs.\\n- With five GSIs, each User update would cost 24 WCUs.\\n- The simple over-reading approach from the prior example is also impractical due to the large User size.\\n- An eventually consistent read of a 4KB user would cost half an RCU. With a thousand customers, this would be 500 RCUs per query.\\n- While the application allows filtering on certain attributes, these make up only a small portion of the record, approximately 100 bytes, not the full 4KB.\\n',\n",
       " '- Secondary indexes and index projections can be used to mitigate data challenges, reducing writes by excluding non-projected attribute changes.\\n- Pete Naylor provides additional information on secondary index projects.\\n- Index projections can also help in reducing item size for complex filtering, hence enhancing efficiency.\\n- If item sizes are reduced to 100 bytes, 40 customers can be fetched per RCU unit.\\n- Fetching 1000 customers only costs 40 RCUs, or 20 for eventually consistent reads, a reduction of 96% in cost.',\n",
       " '- Fetching full records in DynamoDB requires a follow-up, BatchGetItem operation.\\n- This has additional costs for read operations and response latency.\\n- Complex filtering may not always require fetching full records, just a list that matches the filter.\\n- Further actions, such as viewing full records, are often user-initiated.\\n- This operation pattern may highlight when certain use cases are impractical.\\n- If high customer volumes (e.g., 100,000 in CRM) necessitate frequent fetch and filter operations, this can lead to substantial RCUs.\\n- In high RCU scenarios, integrating functional elements like Rockset or Elasticsearch for complex filtering is advised.\\n- These additions can increase cost and complexity, but could provide the right balance for specific use cases.',\n",
       " \"- Summary of the post's content: understanding DynamoDB costs.\\n- Background on DynamoDB billing model.\\n- Examples of evaluating DynamoDB costs in practice.\\n- Key emphasis: leverage DynamoDB's predictability and transparency to:\\n  - Assess cost-effectiveness of access patterns.\\n  - Weigh tradeoffs of different DynamoDB approaches.\\n  - Determine when DynamoDB may not be suitable for certain use cases.\",\n",
       " '- Distinguishing between event-driven architectures and event-based compute in serverless applications.\\n- AWS Lambda is described as event-based compute, impacting code writing and architecture design for serverless applications.\\n- Many serverless applications use an event-driven architecture, relying on decoupled, asynchronous processing of events across the application.\\n- Event-driven architectures and event-based compute are related concepts, often used together in serverless applications on AWS, but they are not the same thing.\\n- Patterns used for one may not apply if the other is not being used.',\n",
       " \"- Lambda's event-based compute nature substantially differentiates it from other computing paradigms.\\n- This distinctiveness results in specific constraints and requirements for serverless application developers.\\n- The post will explore both event-driven architecture and event-based compute.\\n- Key characteristics of each and their implications for applications will be inspected.\",\n",
       " '- Event-driven architectures are currently popular.\\n- These architectures are unique due to asynchronous communication via events.\\n- This distinguishes event-driven architectures from other architecture patterns.',\n",
       " '- Event-driven architectures communicate asynchronously.\\n- The request-response pattern, where a client calls a backend service and waits for a response, is synchronous.\\n- Examples include frontend clients calling a backend GraphQL API, or a backend service calling another service via REST API or RPC.\\n- In a synchronous flow, the client waits for a full response indicating the result of the request.\\n- Synchronous flow is simple as you get direct feedback on the request.',\n",
       " \"- Synchronous communication can affect the application's performance, especially with slower tasks.\\n- It can lower the availability of services as they are interdependent for operation.\\n- Any downtime in Service B will reflect as downtime in Service A if it relies on synchronous responses from Service B.\\n- Asynchronous communication can mitigate these issues as it doesn't expect an immediate response.\\n- The downstream service can process the communication based on its own schedule, freeing up the upstream service.\\n- However, asynchronous communication brings its own challenges such as debugging, eventual consistency, etc.\\n- For further information on the comparison between request-response and event-driven architectures, look for Talia Nassi's post on the advantages of transitioning to event-driven architectures.\",\n",
       " '- Event-driven architectures are characterized by communication through events.\\n- In these architectures, services broadcast events that are consumed and responded to by other services.\\n- Events involve two types of entities:\\n  - Event producers, which publish events representing occurrences within the service (e.g., User creation, Order delivery, Login Attempt failure).\\n  - Event consumers, which subscribe to these events and react accordingly (e.g., updating state, incrementing aggregates, triggering workflows).\\n- Producers and consumers in event-driven architectures are entirely decoupled - a producer is not aware or concerned about who is consuming its events and how those events are utilized by the consumers.',\n",
       " '- An event producer is compared to a news broadcaster who announces news regardless of the audience.\\n- This is different from a traditional message-driven architecture, where a system component sends a message to a queue (such as SQS or RabbitMQ) for processing.\\n- Both message-driven and event-driven patterns are asynchronous.\\n- In a message-driven pattern, the message producer deliberately sends the workload to a specific customer.\\n- The queue aids in resilience and quicker response times from the original component.\\n- However, it\\'s not genuinely \"event-driven\" due to the close relationship between the message producer and consumer.',\n",
       " '- Event-driven architectures are likened to a TV broadcast, while message-driven workflows are compared to a specific email request from a boss asking for a report.\\n- There is some debate over whether message-driven workflows should be categorized as part of event-driven architecture.\\n- Both patterns share similar reasons for use, and any distinctions might not be of great consequence.\\n- Event-driven architectures offer notable flexibility and resilience.\\n- Adding a new consumer to an event-driven architecture does not require coordination with the event producer.\\n- The new consumer can start processing events as they are published for use in a new service.',\n",
       " '- Event-driven architectures have been around for a long time, with developers from the 90s and 2000s having experience with enterprise service buses.\\n- The rise of Apache Kafka and effective evangelism by Jay Kreps (Kafka creator) has revived interest in event-driven architectures using logs and streams.\\n- There are many variations of event-driven architectures, including purist patterns like event sourcing and CQRS.\\n- It is generally recommended to avoid purist patterns as they can become maintenance and debugging challenges, despite being interesting concepts.\\n- This is a brief overview of event-driven architectures.\\n- For more information, AWS Developer Advocate David Boyne is a recommended resource, with extensive content on event-driven architectures, including visuals on Serverless Land.',\n",
       " '- Event-based compute is a concept related to event-driven architectures.\\n- It has two key characteristics:\\n  1. The existence of a compute instance is tied to the occurrence of an event to be processed.\\n  2. The compute acts on a single event at a time.\\n- Example: A Lambda function.\\n  - You write code, create a ZIP file, upload it to AWS, and configure it in the Lambda service.\\n  - This configuration does not start your code running by default.\\n  - No actual instances of your Lambda compute are running initially.\\n  - Your Lambda function has potential but has not realized it yet.',\n",
       " \"- Lambda functions need to be connected to an event source to be invoked.\\n- Popular event sources include API Gateway (HTTP requests), SQS (queue processing), EventBridge (event bus), and Kinesis/DynamoDB Streams (stream processing).\\n- The Lambda docs use the term 'event-driven' for many event sources, even some that may not be considered truly event-driven.\\n- When an event occurs in the configured service, Lambda creates an instance of the function and passes the triggering event to it for processing.\\n- The function processes the event as desired and returns a response back to the event trigger.\",\n",
       " \"- Lambda service may keep a function instance running for a short time to handle other events for optimization.\\n- The control over this behavior is largely outside users' reach.\\n- An instance of running compute in Lambda is primarily created to handle a single specific event.\\n- This separates Lambda from traditional instances or containers, which are made to handle on-demand requests, or to poll for messages from a queue/stream.\\n- It also distinguishes Lambda from scheduling a Fargate task, as the task does not inherently know the event that initiated it while executing.\",\n",
       " \"- Event-based computing with AWS Lambda has significant implications for application architecture.\\n- Engineers using Lambda tend to have certain preferences:\\n  - Stateless functions\\n  - Rapid initialization\\n  - Decoupled architectures\\n  - Asynchronous processing\\n  - Granular decomposition of workloads\\n- These preferences are not universal, but are common in Lambda-based architectures compared to other compute methods.\\n- Two main implications of Lambda's event-based nature:\\n  - Greater emphasis on statelessness and rapid initialization.\\n  - In instance-based or container-based applications, initialization work can be done before handling requests.\\n  - With Lambda, initialization must happen quickly for each invocation.\",\n",
       " '- Lambda compute differs in that it may be initialized in response to an event.\\n- It could involve an active HTTP request from a user waiting on an activity, like viewing tweets or purchasing tickets, requiring the code to load and execute quickly.\\n- This necessitates keeping the function code small and not too layered.\\n- Avoiding an extensive load and initialization of nested dependencies across numerous files is recommended.\\n- Tools like esbuild can be used to bundle code into a single file to decrease initialization-time disk reads, depending on your preference for complexity.',\n",
       " '- Be mindful of ways to enhance performance for subsequent requests.\\n- Certain tasks like establishing network connections or retrieving dynamic configurations are required during the cold-start initialization.\\n- Reusing these resources can prevent every request from behaving like a cold start.\\n- Scaling involves increasing the number of compute instances rather than concurrency within a single instance.\\n- Within a Lambda function instance, only one event is processed at a time.\\n- If multiple events occur simultaneously, the Lambda service creates additional function instances to handle these events.\\n- Every function instance deals with only one event at a time.',\n",
       " \"- The choice of application code writing and potentially even coding language may change.\\n- Ben Kehoe suggested that Node is not suitable for serverless due to its asynchronous-first approach.\\n- This approach works well for traditional backend apps dealing with parallel web requests or batch queue messages processing, contrary to Lambda's one-event-at-a-time model.\\n- Noble's model doesn't usually involve a lot of asynchronous work with Lambda - it processes a single event sequentially.\\n- A more procedural style can benefit single event processing.\\n- Though Node.js has evolved over six years, including async/await features, the single-event model's implications must be considered when choosing a language.\",\n",
       " '- Lambda scales horizontally across function instances, preventing shared resources across concurrent requests within the compute layer.\\n- A common example is a database connection pool, typically shared across many requests on a traditional web server.\\n- This leads Lambda users to prefer databases without connection limits like DynamoDB, or implement database pooling outside their compute, such as Amazon RDS Proxy.',\n",
       " '- For serverless applications, avoid having compute resources idle while waiting, as you are billed for active time.\\n- Avoid using setTimeout() in web servers for background jobs.\\n- Avoid regular long-polling within the function.\\n- If waiting for something before proceeding, try to turn it into an event.\\n- If polling is required, implement the polling trigger outside of the compute function.\\n- The goal is to minimize idle time and optimize billing for active compute time.',\n",
       " \"- There's confusion about the interaction between event-driven architecture and event-based compute, including their usage with AWS Lambda and Kubernetes.\\n- The Lambda documentation, which describes services interacting with Lambda, contributes to this confusion.\\n- The documentation labels certain sources as 'event-driven' that don't match the traditional definition, like HTTP requests from API Gateway.\\n- Likewise, many services using event-driven patterns (DynamoDB Streams, Kinesis Streams, Apache Kafka) are not characterized as event-driven.\\n- A Venn diagram has been prepared to illustrate the overlap between these patterns.\\n- AWS Lambda functions are always categorized under event-based compute.\",\n",
       " \"- AWS Lambda functions can utilize event-driven patterns.\\n- Compute methods other than AWS Lambda can use either event-driven or non-event-driven patterns.\\n- The text doesn't cover all scenarios.\\n- Other tools like OpenFaaS and Knative also allow for event-based compute.\\n- The author has less familiarity with these, but assumes many of the same principles would apply.\",\n",
       " '- Emily Shea suggested clarification on why some AWS services may or may not be event-driven with Lambda.\\n- A brief overview regarding the interaction of these services with Lambda is provided.\\n- API Gateway + Lambda: Generally, this is not event-driven. It follows a request-response pattern where the client gets a response from the Lambda function after a request.\\n- Traditionally, like in REST API operations (e.g., \"Get User\", \"Add to Cart\"), API Gateway is not event-driven but is synchronous and tightly coupled.',\n",
       " '- API Gateway can act as an entry point to an event-driven system. \\n- A service or frontend client can publish an event to an API Gateway endpoint which goes on to a Kinesis Stream or EventBridge bus.\\n- The response code or body can serve as an indication of whether the API is a frontend to an event-driven system.\\n- If a response code of 202 Accepted or a response body containing an eventId or messageId property is received, it could suggest the API being a frontend to an event-driven system.\\n- SQS + Lambda is typically not event-driven but asynchronous.\\n- However, this differs from the synchronous request-response pattern.\\n- In a queue-based pattern, the producer usually has a specific task in mind (e.g. \"Send a welcome email after user creation.\").\\n- This is referred to as a \"message-driven\" pattern and might also be called \"point-to-point\".',\n",
       " \"- SQS could be utilized as a buffering/throttling mechanism in an event-driven system, like a consumer to SNS or EventBridge may push messages to SQS for processing.\\n- Using SQS can offer better control and visibility over throttling and retry logic.\\n- SNS and Lambda, in theory, can create an event-driven system. An SNS topic can have multiple consumers, and a publisher to an SNS topic doesn't need to anticipate a specific outcome.\\n- Similarly, EventBridge and Lambda can create an event-driven system. EventBridge, with its focus on an event bus with diverse event types, could be even more likely to be event-driven than SNS.\\n\",\n",
       " '- Kinesis and Lambda are often used in event-driven architectures, similar to SNS and EventBridge.\\n- Unlike SNS and EventBridge, Kinesis allows for batch processing of events.\\n- Kinesis may not be as suitable for pure event-driven architectures due to its batch processing capability.\\n- Step Functions and Lambda are not event-driven, and instead execute defined multi-step workflows.\\n- Even though Step Functions are not event-driven, events can be published between steps of the state machine execution.\\n- The post acknowledges Emily Shea for suggested content and Maik WiesmÃ¼ller for contributions to the original text.',\n",
       " '- The post covered the differences between event-driven architectures and event-based compute.\\n- The benefits of event-driven architectures can be leveraged across various types of compute, given the right conditions.\\n- All uses of Lambda depend on event-based compute.\\n- The implications of event-based compute and its impact on application design have been discussed.\\n- Feedback, questions, or corrections regarding the post are welcome.',\n",
       " \"- Background and biases framing the author's perspective on Serverless Framework vs AWS CDK.\\n- Four reasons why the Serverless Framework is considered a better abstraction for building serverless applications on AWS.\\n- Areas where the CDK performs well.\\n- Comparison of the Serverless Framework and AWS CDK for serverless application development on AWS.\\n- Contrasting views from other developers on their experiences with the CDK.\\n- The author's preference for the Serverless Framework over the CDK, despite the CDK's growing popularity.\",\n",
       " \"- Background and personal preferences influence the author's views.\\n- The author worked for Serverless, Inc., creators of the Serverless Framework, for around 2.5 years.\\n- This experience provided significant knowledge about AWS and serverless, particularly with the Serverless Framework.\\n- Although no longer financially invested, the author has friends at Serverless, Inc. and an affinity for the company.\\n- Personal circumstances may differ, so some points may not be applicable to all readers.\",\n",
       " '- The focus is on developing small scale serverless applications on AWS.\\n- The term \"serverless\" is commonly interpreted as applications primarily using managed services combined with AWS Lambda functions.\\n- Acknowledgement that Lambda does not equate to serverless, and one can create functionless serverless applications.\\n- There is some caution towards functionless applications, but it is a viable option.\\n- Despite variations, most serverless applications on AWS feature Lambda at the core.',\n",
       " '- The applications discussed rarely use a VPC due to its complex, boilerplate configuration.\\n- There are no security groups, autoscaling pools, or ECS service definitions.\\n- The need to create similar yet subtly different resources is not present, which could otherwise be an area where the CDK offers help.',\n",
       " '- Preference for simplicity and avoiding complexity in programming (\"boring\" approach).\\n- Advocating AWS Lambda and DynamoDB as relatively straightforward technologies with understandable operating and failure models.\\n- Favoring explicit over implicit coding, stemming from Python background.\\n- Avoiding excessive indirection or cleverness in programming.\\n- Prioritizing simplicity to benefit future self.\\n- Acknowledging personal biases and inviting readers to consider them when evaluating the content.',\n",
       " '- Both Serverless Framework and AWS CDK are pre-processors for AWS CloudFormation.\\n- The core difference lies in their approach to generating CloudFormation templates.\\n- Serverless Framework uses a declarative YAML configuration file called serverless.yml to describe the application.\\n- The serverless.yml file contains top-level blocks for functions (Lambda) and resources (other AWS services).\\n- It provides a standard structure for defining serverless applications.',\n",
       " '- The key abstraction lies in the functions block, which simplifies creating Lambda functions and connecting event sources to them.\\n- This is the core of most serverless applications, and simplifying this process is a significant benefit.\\n- The CDK takes a different approach by using imperative code in a general-purpose programming language (TypeScript, Python, Golang, etc.) to describe the desired infrastructure, instead of a declarative configuration file.\\n- With this structure, you can use functions to abstract common operations, loops to create multiple resources with similar configurations, and abstracted modules from other files or NPM packages to encapsulate resource creation logic.',\n",
       " \"- The Serverless Framework offers an understanding of the application structure with the help of a serverless.yml file. This can reveal functions, their location in the repo, triggers, and used resources.\\n- The framework operates on a 'convention over configuration' approach, similar to the Rails ecosystem.\\n- Adhering to conventions allows easy comprehension of a project even after weeks, as seen in the use of Laravel applications.\",\n",
       " \"- CDK repositories can be complex to navigate, with application and infrastructure code intertwined.\\n- Locating specific code, such as function handlers or service code, can be challenging.\\n- There are concerns about long-term maintenance issues with CDK, like the updating of constructs or version updates.\\n- A bigger concern is maintaining developer knowledge for a specific CDK application.\\n- Onboarding a new developer into a custom CDK structure has challenges because they need to grasp the team's specific implementation.\\n- The team's hidden knowledge of the application could lead to slower onboarding times for new developers.\",\n",
       " '- The Serverless Framework and the CDK are both abstractions over CloudFormation.\\n- The Serverless Framework is believed to be superior, offering easier handling of serverless applications.\\n- The complexity of deploying serverless applications is acknowledged.\\n- The Serverless Framework focuses on functions and events, considering them as core to serverless applications.\\n- An example functions block in a serverless.yml file will be examined next.',\n",
       " \"- Running serverless deploy builds and configures an AWS function for each function in the functions block (createUser, processQueue, and fanout are given as examples).\\n- The process may include installing dependencies, running a build process, zipping the directory, uploading it to S3, and registering the function with basic configurations on the Lambda service.\\n- Understanding the packaging process is not crucial unless there are specific needs.\\n- The CDK's function abstractions, such as the NodeJsFunction construct, also perform most of these tasks.\\n- There's an opinion that the function build part of CDK is not as well-developed as the core infrastructure deployment part, though it's functional.\\n- A user named Roman found it problematic to build, test, and publish TypeScript lambdas using lambda constructs.\",\n",
       " '- Configuring events to trigger functions is a key challenge in serverless applications.\\n- The serverless.yml example shows a consistent format for potential triggers.\\n- Each function has an events property to configure an array of events (usually one per function).\\n- Each event has a type and required properties specified.\\n- Configuring a Lambda function for API Gateway HTTP requests is similar to configuring for SQS queues or SNS notifications.\\n- These configurations can be complex, e.g., API Gateway requires 6-8 resources like sub-resources, IAM role, and permissions.',\n",
       " '- The Serverless Framework abstracts complex tasks without causing disadvantages, which will be discussed further.\\n- Comparing the configuration of three event types in the CDK reveals three distinct management methods.\\n- API Gateway requires creating the REST API instance, creating a resource on the API, and adding a method on the created resource that integrates with the Lambda function.\\n- For SQS integration, the format changes entirely. Certain Lambda integrations use event source mappings, exposure of which is done by the CDK.\\n- Thus, connecting a Lambda function to an SQS queue follows a different process.\\n- Source: CDK docs.',\n",
       " '- Lambda functions are created, along with an SQS Queue.\\n- A third element, an event source mapping, is created to connect the Lambda function and SQS Queue, attached to the Lambda function.\\n- For SNS topic integration, a Lambda function and an SNS topic are created.\\n- The Lambda function is attached to the SNS topic, rather than the other way around like with SQS.\\n- In all three cases, CDK accurately represents the resources required for these integrations.\\n- However, there is complexity that could be abstracted away, such as event source mappings vs. API Gateway Lambda integrations.\\n- While the specifics of error handling across different services need to be understood, the details of connecting resources may not always be necessary.',\n",
       " '- The typical way of building serverless applications on AWS can be confusing.\\n- The build should be Lambda-centric or compute-centric if going functionless, using numerous managed services tied together via Lambda.\\n- In the Serverless Framework, Lambda functions and events are the core and the additional resources are secondary.\\n- CDK treats all resources equally, which can create organizational challenges for your application functioning.',\n",
       " '- The Cloud Development Kit (CDK) may not abstract the optimal features.\\n- The previous section possibly oversimplified the Serverless Framework event configuration.\\n- The SQS and SNS integrations reference additional infrastructure in an application, namely, an SQS queue and an SNS topic.\\n- The Serverless Framework acknowledges the need for extra supporting infrastructure for serverless applications through a resources block in serverless.yml.\\n- The resources block allows users to configure any additional AWS resources through CloudFormation.',\n",
       " '- Writing CloudFormation is a beneficial aspect as it helps learn the underlying tool.\\n- Common complaints about CloudFormation relate to its verbosity and extensive boilerplate.\\n- Much of the boilerplate in CloudFormation is abstracted by functions and events.\\n- The resources created are straightforward, with little scope for abstraction in SQS queues, SNS topics, and DynamoDB tables.\\n- AWS SAM attempted abstraction with the SimpleTable resource for DynamoDB, but the other properties are useful.\\n- Lambda functions require IAM permissions to interact with resources, which need to be specified in IAM-policy format in the iam block in serverless.yml.',\n",
       " '- CDK abstracts away raw CloudFormation and IAM details, allowing developers to invoke methods with properties instead of writing declarative YAML.\\n- For granting permissions, CDK simplifies the process by using methods like grantReadWriteData to allow a function access to a DynamoDB table, rather than specifying individual IAM statements.\\n- However, at some point, developers need to understand the underlying fundamentals of CloudFormation and IAM.\\n- If a deployment fails, the error will be specific to CloudFormation, requiring knowledge of its terminology and concepts for effective debugging.\\n- Similarly, if permissions are incorrect, the error will reference specific resources and IAM actions, which may be unfamiliar to those without prior knowledge.\\n- AWS is a vast ecosystem with thousands of services, intricate connections, complex authentication requirements, and numerous pitfalls, making it truly daunting for newcomers.\\n- While CDK provides an abstraction layer, understanding the underlying fundamentals is essential for effectively working with AWS and troubleshooting issues.',\n",
       " '- The Serverless Framework enabled significant AWS skills improvement.\\n- It allows for a quick start with a simple function and event connection, without needing to learn CloudFormation and IAM details initially.\\n- As needs grow, new components like DynamoDB tables and associated permissions can be incrementally added.\\n- Learning happens gradually, as new features are introduced, rather than being overwhelmed by complexity upfront.\\n- The incremental approach avoids being thrown into the deep end when issues arise.',\n",
       " '- The CDK may encourage harmful tendencies.\\n- The Serverless Framework provides helpful structure, which is beneficial even for non-beginners.\\n- The ability to code your infrastructure is a double-edged sword in the CDK.\\n- Real-world CDK applications may feature elements that cause concerns.\\n- Engineers can unnecessarily complicate things with abstraction, excessive focus on DRY principles, or obsession over writing aesthetically pleasing code.\\n- These sophisticated constructs can create barriers for newcomers or make long-term maintenance difficult.',\n",
       " '- There are a couple of prominent issues with the CDK, although other issues may exist.\\n- The first issue is the potential for a clutter of resources without comprehension of their underlying purpose or intent.\\n- An example of this issue is the common occurrence of an undue increase in CloudFormation Stacks.\\n- An illustration is a CDK application that used seven different stacks for a relatively minor service involving around 8 HTTP endpoints, an S3 bucket, a DynamoDB table, and several state machine definitions.\\n- These stacks were not CloudFormation Nested stacks, which are inherently interlinked, but were separate, independent stacks leading to complexity in deployment and deployment failures.',\n",
       " '- The second issue is overemphasizing reusable abstractions across teams, a core CDK concept with different levels of constructs.\\n- People believe they can write a single resource to solve all problems, but reality is more complex.\\n- The average software developer, including DevOps and full-stack engineers, struggles to write truly reusable, library code.\\n- Few developers possess deep knowledge of cloud infrastructure combined with the ability to create useful abstractions.',\n",
       " '- Taylor Otwell, the creator of Laravel, shared a screenshot of a Hacker News discussion related to Laravel projects.\\n- The discussion highlighted the difficulty in creating good, solid abstractions.\\n- Otwell acknowledged that the points made in the discussion are likely true for Laravel projects as well.\\n- Creating effective abstractions is challenging, but the temptation to try is strong.\\n- In most cases, it is better to rely on proven, tested abstractions rather than attempting to write new ones from scratch.',\n",
       " '- The CDK (AWS Cloud Development Kit) is gaining significant adoption despite some criticisms.\\n- Benefits of the CDK:\\n  - Removes drudgery of infrastructure-as-code\\n    - Writing YAML can be boring\\n    - Imperative coding can be more enjoyable\\n    - Benefits of auto-complete and L2 constructs for low-level gluing\\n  - However, the imperative approach can be more verbose and complex',\n",
       " \"- Fun as a benefit must be approached with caution as it doesn't necessarily equate to quality.\\n- The Serverless Framework simplifies many aspects of small serverless applications.\\n- The drive to use the CDK, especially for experienced AWS engineers familiar with IAM, CloudFormation, and the IaC process, is recognized as a valid reason.\\n- Another advantage of CDK is that it simplifies the process of creating many similar resources.\\n- As highlighted in the aws.fm podcast, using CDK can reduce the tedium of creating hundreds of similar RDS instances, providing an easier alternative to a more declarative format.\",\n",
       " '- Encountered a single issue in approximately six years of serverless development related to maintaining resources with similar instances. \\n- Addressing the issue was not particularly demanding. \\n- A significant advantage is the ability to abstract repetitive elements. \\n- When creating solutions like a VPC that require numerous configured resources, abstraction into a single construct with reasonable defaults is highly beneficial. \\n- However, the ownership of the infrastructure remains with the user, despite simplifications offered by the CDK.\\n- A relevant tweet from Gwen Shapira is mentioned.',\n",
       " '- AWS EKS \"get started\" documentation recommends running CloudFormation templates.\\n- These templates create numerous resources, mostly network-related, without explaining their purpose.\\n- When issues arise, it becomes challenging to debug or assist others due to lack of understanding.\\n- Using off-the-shelf solutions without learning the underlying concepts can lead to difficulties down the line.\\n- The AWS CDK (Cloud Development Kit) promotes experimentation and exploration of new approaches.\\n- Technology and software evolve through trial and error, making it difficult to predict what will work.\\n- Initial skepticism or hostility towards new trends may sometimes be proven wrong.\\n- Infrastructure as code is still an unsolved problem, and various approaches should be explored.',\n",
       " \"- The post explains the preference for the Serverless Framework for building serverless applications on AWS, due to the way it constrains impulses, leans towards convention over configuration, and how it educates users about the cloud.\\n- The CDK's strengths were also briefly acknowledged.\\n- The conclusions drawn are subjective and based on the author's experiences and preferences; different perspectives may lead to different conclusions.\\n- The author appreciates alternative views and invites feedback.\\n- Errors in the post are the author's responsibility.\\n- Any questions or corrections can be directed to the author via comment or email.\\n- The post also acknowledges and thanks Matt Bonig for his input and for his co-authored resource, The CDK Book.\",\n",
       " \"- The 2007 Dynamo Paper from Amazon engineers helped launch the NoSQL movement and led to the creation of databases like Apache Cassandra, MongoDB, and DynamoDB.\\n- A new paper on DynamoDB has been released by Amazon, providing insights into how the core concepts from Dynamo were adapted for a fully managed, highly scalable, multi-tenant cloud database service.\\n- The post highlights key takeaways from the new DynamoDB Paper, focusing on two main areas of interest.\\n- The post aims to provide a higher-level overview of the paper, while recommending readers to review the original paper and Marc Brooker's review for additional details.\\n- Background: The Dynamo Paper described an internal database used by Amazon to handle the enormous scale of its retail operation, leading to the development of DynamoDB and other NoSQL databases.\",\n",
       " \"- The Dynamo and DynamoDB papers detail impressive technical concepts along with in-depth discussions on user needs.\\n- Both papers examine established practices to evaluate their significance and potential need for reconsideration.\\n- The Dynamo paper revealed the lesser use of certain higher-level querying functions of an RDBMS within Amazon's services.\\n- Werner Vogels later emphasized that 70% of database operations were single-record primary key lookups and another 20% read multiple rows from a single table.\",\n",
       " \"- The original Dynamo paper highlighted that the strong consistency guarantee isn't necessary for all applications.\\n- Enhanced availability and reduced write latency achieved by relaxing consistency requirements can provide significant benefits.\\n- The DynamoDB paper re-assessed traditional database systems concepts and evaluated user needs to broaden Dynamo's application.\\n- As a result, DynamoDB established unique product priorities setting it apart from many other databases.\\n- The paper provides three critical observations on user needs.\",\n",
       " '- Consistent performance at any scale is often more important than median request service times for many users.\\n- It\\'s better to have a narrower range between median and tail latency than reducing median, p90, or p95 latency.\\n- DynamoDB\\'s performance story is more nuanced than just being \"super fast.\"\\n- RDBMS latency worsens as the amount of data in the database increases, while DynamoDB provides consistent performance as data grows.\\n- The same relationship holds as the number of concurrent requests increases.\\n- The key point is DynamoDB\\'s consistent performance at scale, versus varying latency in RDBMS as data and concurrency grow.',\n",
       " \"- At certain data and transaction volumes, an RDBMS may have faster response times than DynamoDB.\\n- This is conceptually understandable, as a DynamoDB request passes through multiple systems (request router, metadata system, authentication system) before reaching the physical storage node, adding latency.\\n- In contrast, a single-node RDBMS can operate directly on local data, avoiding much of this overhead.\\n- While MySQL may outperform DynamoDB at the median, it's important to consider the full spectrum of database performance for two reasons.\",\n",
       " \"- Tail latencies are crucial, especially in architectures with many components and sub-components.\\n- If a single backend call triggers multiple calls to underlying services, the request is more likely to experience tail latency from some service, resulting in slower responses.\\n- DynamoDB's consistent and predictable performance profile reduces long-term maintenance burden on the service.\\n- There is no need to investigate, tune, and refactor as performance declines over time.\\n- The performance in the test environment will match the performance years after launch, allowing focus on value-generating features for users.\",\n",
       " \"- The topic discusses the advantages of fully managed services over self-managed services in cloud and serverless contexts.\\n- Serverless environments aim to focus on unique business elements and delegate undifferentiated tasks.\\n- The transformation within Amazon retail and the creators of Dynamo (not DynamoDB) illustrates this idea.\\n- The Dynamo system had a substantial impact and significantly improved Amazon's scalability and availability at a vast scale.\\n- Despite these advancements, many engineers preferred using Amazon SimpleDB (AWS's initial entry into the hosted NoSQL database market) over managing Dynamo themselves.\",\n",
       " \"- Amazon SimpleDB is a lesser-known service; it is succeeded, and in most aspects, surpassed, by DynamoDB.\\n- Despite being less promoted by AWS, SimpleDB signifies AWS' policy of never deprecating a service, even if superior ones exist.\\n- SimpleDB has major drawbacks, one of them is a strict 10GB size limit for the entire database.\\n- This limit was a significant issue, especially for large-scale applications - yet, engineers opted to use multiple SimpleDB tables, likely implementing sharding at the application layer.\\n- While this added complexity to the application logic, engineers still preferred SimpleDB over operating their own Dynamo instance.\\n- This pattern influenced the development of Amazon DynamoDB, a database merging the scalability of Dynamo with the fully managed characteristic of SimpleDB.\",\n",
       " \"- User data often isn't distributed as evenly as desired.\\n- Ideally, users have stable and predictable traffic that evenly distributes data access across keyspaces, but actual scenarios differ.\\n- The original Dynamo paper outlined consistent hashing to distribute data across around 10GB-sized independent partitions.\\n- This system uses an item's partition key to organize data across partitions, enabling predictable performance and seamless horizontal scaling.\\n- Unlike the original Dynamo system, DynamoDB is a multi-tenant system.\\n- Partitions of different DynamoDB users can be co-located.\",\n",
       " '- Initially, DynamoDB had a system to prevent noisy neighbor issues, where high traffic to one partition could impact other unrelated partitions on the same storage node.\\n- However, this initial system led to a subpar experience for workloads with spiky or unbalanced traffic patterns.\\n- As DynamoDB evolved, AWS realized the need to enhance the access control system that managed whether a partition could service a request.\\n- The technical aspects of this evolution are explored further below.',\n",
       " '- The DynamoDB paper provides interesting technical learnings.\\n- The work done by the DynamoDB team, even at a massive scale, can be applied to smaller scales.\\n- There are three key technical takeaways from the paper.',\n",
       " \"- DynamoDB uses log replicas to improve durability and availability during instance failures\\n- To understand log replicas, we need background on DynamoDB's storage architecture\\n- DynamoDB splits data into partitions, which are independent 10GB storage segments\\n- The partition key is used to assign items to a given partition\\n- This allows DynamoDB to scale horizontally as the database grows while keeping related items together\\n- DynamoDB runs a large fleet of storage nodes handling partitions from many different user tables\",\n",
       " '- An individual partition in DynamoDB is a set of three partition instances across different availability zones, forming a replication group.\\n- One instance acts as the leader for a given partition, responsible for handling all writes.\\n- When a write operation occurs, the leader writes the data locally and ensures it is committed to at least one additional replica before responding to the client.\\n- This replication approach increases durability, as the loss of one node will not result in data loss.',\n",
       " '- Every storage partition contains two data structures: the B-tree of indexed data and a write-ahead log (WAL) listing updates applied to the partition.\\n- Write-ahead logs, commonly used in databases, enhance the durability and latency of write operations.\\n- Updating the B-tree is slower due to random I/O and potential re-writing of multiple disk pages.\\n- Write-ahead log updates, being append-only, are much faster.\\n- The concept of write-ahead logs is fundamental to Kafka and similar systems.',\n",
       " \"- Individual operations against B-trees and write-ahead logs present a significant performance difference.\\n- There's a considerable size difference between the two structures. The B-tree can be over 10GB in size (included the partition's 10GB size and the index overhead), while the write-ahead log is merely a few hundred megabytes.\\n- The write-ahead log's full history is periodically synced to S3, enabling point-in-time restore and other features.\\n- This concludes the section on DynamoDB storage background.\",\n",
       " '- In large systems like DynamoDB, instances frequently fail.\\n- When a storage node fails, thousands of partition replicas need to be relocated to a non-failing node.\\n- During the failure period, every replication group with a replica on that node is down to two replicas.\\n- Two replicas are required to acknowledge a write, so write latency increases and the probability of an availability event rises if another replica fails.\\n- To reduce the time with only two live replicas, DynamoDB uses log replicas.\\n- A log replica is a replication group member containing only the write-ahead log.\\n- By skipping the B-tree for the partition, DynamoDB can quickly spin up a log replica by copying the last few hundred MB of the log.\\n- The log replica can acknowledge writes but not serve reads.',\n",
       " '- DynamoDB uses a log replica to temporarily assist the replication group while a full replica member is brought up to replace a failed one.\\n- The DynamoDB team has continuously made incremental improvements to enhance durability, availability, and latency.\\n- These improvements do not alter the core promises of the system, such as the choice made in the CAP theorem.\\n- They represent steady enhancements to the reliability and performance of the database.\\n- DynamoDB employs a standard combination of a write-ahead log with indexed storage to improve durability and reduce write request latency.',\n",
       " '- DynamoDB uses a partitioned system instead of a single-node setup like RDBMSs.\\n- This partitioning reduces recovery time and improves availability, as recovering a smaller partition is faster than a large table.\\n- The data is replicated across three availability zones, ensuring availability even if an entire zone fails.\\n- DynamoDB relaxes consistency requirements, only requiring two out of three replicas to acknowledge writes.\\n- This trade-off of occasional stale data allows for improved write availability and reduced latency.\\n- DynamoDB utilizes log replicas to further enhance availability during node failures.',\n",
       " '- Partitions in a table are used for data segmentation and horizontal scaling.\\n- DynamoDB places partitions from different customers on the same storage nodes to increase service efficiency.\\n- DynamoDB has gradually improved its \"admission control\" system, the process to check if a request can proceed based on available capacity.\\n- The admission control checks capacity across two different factors:\\n  - Making sure the provided service is properly paid for.\\n  - Avoiding performance issues due to the co-location of partitions.',\n",
       " \"- Initially, DynamoDB's admission control was at the partition level.\\n- Total provisioned throughput was divided equally among partitions.\\n- This was a simple system without coordination across partitions.\\n- However, it led to issues with unbalanced workloads and throughput dilution.\\n- Hot partitions could be throttled even when the table was underutilized.\\n- To address this, DynamoDB aimed to decouple admission control from partitions.\\n- But this was a significant change, so they moved in stages.\",\n",
       " '- There were improvements made to the partition-level admission control system.\\n- Each partition was restricted to prevent overuse of resources on a single node.\\n- Storage nodes were often found to run under full capacity.\\n- To handle temporary traffic increases to individual partitions, short-term bursting was added to DynamoDB, letting a partition use extra throughput if available on the storage node.\\n- This improvement mainly addressed the second axis of access control â€“ protection from noisy neighbors.',\n",
       " '- A second improvement addressed the provisioned throughput for an individual table, another aspect of access control.\\n- Skewed access patterns could lead to a table consuming all the throughput for one partition while remaining below the total provisioned throughput for the table.\\n- DynamoDB introduced adaptive capacity to allow throughput from sparsely used partitions to be redirected to highly used partitions.\\n- These modifications, while preserving the general partition-based access control scheme, significantly reduced issues caused by uneven data access patterns.',\n",
       " \"- DynamoDB transitioned to a global access control system, decoupling throughput from partitions.\\n- This changed adaptive capacity from a slower, 'best efforts' approach to a near-instant system for distributing throughput across partitions.\\n- This flexibility enabled several improvements:\\n  - Separating hot items onto their own partitions\\n  - Introducing DynamoDB On-Demand billing\\n  - Overloading storage nodes based on predicted partition workloads\\n- Section 4 of the referenced paper provides more detailed information on these changes and is recommended for further understanding.\",\n",
       " '- DynamoDB utilizes asynchronous caches.\\n- Asynchronous caches mean a system that stores data locally and refreshes it in the background to keep it updated.\\n- Caching is used to decrease latency by saving the results of costly calls.\\n- In scenarios given in the study, individual request router objects store the outcomes of external calls locally to evade a slow network query.\\n- Two interesting points can be observed when reviewing DynamoDB\\'s treatment of \"external\" (or \"dependencies\") versus \"internal\" systems.',\n",
       " \"- DynamoDB utilizes other AWS services like IAM for authentication and KMS for encryption/decryption, which are external dependencies not controlled by the DynamoDB team.\\n- DynamoDB caches the results of calls to these external services and periodically refreshes them asynchronously to maintain availability, even if the external services experience issues.\\n- Without caching, DynamoDB's availability would be limited by the availability of services like IAM and KMS.\\n- DynamoDB also employs asynchronous caches for internal systems.\\n- DynamoDB has a metadata system that tracks table information and partition locations.\\n- When a request arrives at a DynamoDB request router, it needs to find the relevant partition for the item to forward the request to the appropriate storage node.\",\n",
       " \"- Metadata information doesn't frequently change, leading to heavy caching by request routers.\\n- The paper states a 99.75% cache hit rate, which is exceptionally high.\\n- A high cache hit rate can lead to issues, such as increased service load due to minor traffic reductions.\\n- Lowering the metadata cache hit rate from 99.75% to 99.5% doubles requests to the underlying metadata service.\\n- The DynamoDB team observed that the metadata service had to scale with the request router service.\\n- New request routers with empty caches caused many calls to the metadata service, destabilizing the overall system.\",\n",
       " '- DynamoDB employs asynchronous cache refreshing to enhance the resiliency of its internal systems and maintain a constant load on the underlying metadata system.\\n- While request routers cache data locally with a high hit rate, each cache hit triggers an associated asynchronous request to the metadata service to refresh the cached data.\\n- By coupling a local cache hit with an asynchronous request to the metadata service, DynamoDB ensures a consistent rate of traffic to the metadata service.\\n- Both cache hits and cache misses result in a request to the metadata service, preventing bursts of new traffic to the metadata service when adding new request routers with cold caches.',\n",
       " '- Metadata caching system has interesting details not covered here.\\n- Two uses of asynchronous caches are highlighted.\\n- Local, instance-based caching was used to reduce latency.\\n- Asynchronous refreshing was coupled with local caching.\\n- Asynchronous refreshing helped decouple availability from external dependencies.\\n- Asynchronous refreshing increased resiliency of internal services.',\n",
       " '- Conclusion: Amazon continues to advance understanding of deep technical topics, as seen in the influential Dynamo and DynamoDB papers.\\n- The post covered core user-needs learnings from the DynamoDB paper.\\n- It also examined three key technical learnings from the paper.\\n- Other interesting points not covered include:\\n  - Monitoring client-side availability through instrumentation of internal Amazon services\\n  - Strategies for deploying new versions against a massive instance fleet\\n  - Mechanisms to protect against data errors in transit and at rest\\n- Invitation for further examination and discussion of these points.\\n- Request for questions, corrections, or feedback on the post.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.preprocessing.neutralising.llm import NeutraliserLLM\n",
    "\n",
    "input_file_path = 'articles/chunks/chunks.txt'\n",
    "output_dir = 'articles/neutralised/'\n",
    "output_style = 'draft'\n",
    "\n",
    "neutraliser = NeutraliserLLM(input_file_path, output_dir, output_style)\n",
    "neutraliser.neutralise_corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: Length 681. Tokens 148\n",
      "Chunk 1: Length 583. Tokens 123\n",
      "Chunk 2: Length 787. Tokens 179\n",
      "Chunk 3: Length 550. Tokens 123\n",
      "Chunk 4: Length 651. Tokens 135\n",
      "Chunk 5: Length 969. Tokens 211\n",
      "Chunk 6: Length 821. Tokens 168\n",
      "Chunk 7: Length 615. Tokens 141\n",
      "Chunk 8: Length 923. Tokens 207\n",
      "Chunk 9: Length 992. Tokens 188\n",
      "Chunk 10: Length 676. Tokens 128\n",
      "Chunk 11: Length 524. Tokens 106\n",
      "Chunk 12: Length 952. Tokens 213\n",
      "Chunk 13: Length 795. Tokens 173\n",
      "Chunk 14: Length 534. Tokens 109\n",
      "Chunk 15: Length 953. Tokens 214\n",
      "Chunk 16: Length 895. Tokens 179\n",
      "Chunk 17: Length 790. Tokens 175\n",
      "Chunk 18: Length 561. Tokens 120\n",
      "Chunk 19: Length 920. Tokens 176\n",
      "Chunk 20: Length 343. Tokens 70\n",
      "Chunk 21: Length 629. Tokens 145\n",
      "Chunk 22: Length 890. Tokens 187\n",
      "Chunk 23: Length 841. Tokens 185\n",
      "Chunk 24: Length 558. Tokens 113\n",
      "Chunk 25: Length 861. Tokens 190\n",
      "Chunk 26: Length 957. Tokens 180\n",
      "Chunk 27: Length 830. Tokens 166\n",
      "Chunk 28: Length 878. Tokens 186\n",
      "Chunk 29: Length 746. Tokens 161\n",
      "Chunk 30: Length 726. Tokens 148\n",
      "Chunk 31: Length 1090. Tokens 219\n",
      "Chunk 32: Length 975. Tokens 195\n",
      "Chunk 33: Length 887. Tokens 195\n",
      "Chunk 34: Length 664. Tokens 138\n",
      "Chunk 35: Length 780. Tokens 172\n",
      "Chunk 36: Length 667. Tokens 137\n",
      "Chunk 37: Length 966. Tokens 187\n",
      "Chunk 38: Length 859. Tokens 177\n",
      "Chunk 39: Length 985. Tokens 206\n",
      "Chunk 40: Length 921. Tokens 184\n",
      "Chunk 41: Length 469. Tokens 87\n",
      "Chunk 42: Length 814. Tokens 177\n",
      "Chunk 43: Length 711. Tokens 137\n",
      "Chunk 44: Length 797. Tokens 154\n",
      "Chunk 45: Length 327. Tokens 66\n",
      "Chunk 46: Length 666. Tokens 130\n",
      "Chunk 47: Length 799. Tokens 172\n",
      "Chunk 48: Length 1041. Tokens 223\n",
      "Chunk 49: Length 618. Tokens 125\n",
      "Chunk 50: Length 964. Tokens 180\n",
      "Chunk 51: Length 342. Tokens 61\n",
      "Chunk 52: Length 784. Tokens 159\n",
      "Chunk 53: Length 696. Tokens 143\n",
      "Chunk 54: Length 590. Tokens 120\n",
      "Chunk 55: Length 762. Tokens 172\n",
      "Chunk 56: Length 772. Tokens 184\n",
      "Chunk 57: Length 881. Tokens 192\n",
      "Chunk 58: Length 782. Tokens 161\n",
      "Chunk 59: Length 384. Tokens 71\n",
      "Chunk 60: Length 420. Tokens 91\n",
      "Chunk 61: Length 1237. Tokens 260\n",
      "Chunk 62: Length 986. Tokens 201\n",
      "Chunk 63: Length 964. Tokens 193\n",
      "Chunk 64: Length 309. Tokens 58\n",
      "Chunk 65: Length 557. Tokens 115\n",
      "Chunk 66: Length 507. Tokens 92\n",
      "Chunk 67: Length 909. Tokens 206\n",
      "Chunk 68: Length 394. Tokens 79\n",
      "Chunk 69: Length 838. Tokens 173\n",
      "Chunk 70: Length 926. Tokens 193\n",
      "Chunk 71: Length 635. Tokens 139\n",
      "Chunk 72: Length 396. Tokens 85\n",
      "Chunk 73: Length 888. Tokens 183\n",
      "Chunk 74: Length 899. Tokens 194\n",
      "Chunk 75: Length 776. Tokens 167\n",
      "Chunk 76: Length 856. Tokens 181\n",
      "Chunk 77: Length 615. Tokens 121\n",
      "Chunk 78: Length 945. Tokens 197\n",
      "Chunk 79: Length 637. Tokens 138\n",
      "Chunk 80: Length 793. Tokens 175\n",
      "Chunk 81: Length 614. Tokens 130\n",
      "Chunk 82: Length 938. Tokens 196\n",
      "Chunk 83: Length 447. Tokens 101\n",
      "Chunk 84: Length 944. Tokens 192\n",
      "Chunk 85: Length 817. Tokens 182\n",
      "Chunk 86: Length 783. Tokens 200\n",
      "Chunk 87: Length 952. Tokens 235\n",
      "Chunk 88: Length 920. Tokens 215\n",
      "Chunk 89: Length 644. Tokens 161\n",
      "Chunk 90: Length 753. Tokens 189\n",
      "Chunk 91: Length 806. Tokens 171\n",
      "Chunk 92: Length 874. Tokens 197\n",
      "Chunk 93: Length 441. Tokens 99\n",
      "Chunk 94: Length 962. Tokens 221\n",
      "Chunk 95: Length 661. Tokens 162\n",
      "Chunk 96: Length 776. Tokens 176\n",
      "Chunk 97: Length 777. Tokens 195\n",
      "Chunk 98: Length 836. Tokens 195\n",
      "Chunk 99: Length 936. Tokens 243\n",
      "Chunk 100: Length 385. Tokens 87\n",
      "Chunk 101: Length 883. Tokens 201\n",
      "Chunk 102: Length 890. Tokens 227\n",
      "Chunk 103: Length 670. Tokens 149\n",
      "Chunk 104: Length 983. Tokens 221\n",
      "Chunk 105: Length 607. Tokens 132\n",
      "Chunk 106: Length 900. Tokens 181\n",
      "Chunk 107: Length 409. Tokens 77\n",
      "Chunk 108: Length 304. Tokens 65\n",
      "Chunk 109: Length 541. Tokens 106\n",
      "Chunk 110: Length 1029. Tokens 193\n",
      "Chunk 111: Length 889. Tokens 183\n",
      "Chunk 112: Length 763. Tokens 153\n",
      "Chunk 113: Length 874. Tokens 184\n",
      "Chunk 114: Length 1023. Tokens 209\n",
      "Chunk 115: Length 877. Tokens 208\n",
      "Chunk 116: Length 868. Tokens 182\n",
      "Chunk 117: Length 739. Tokens 149\n",
      "Chunk 118: Length 997. Tokens 195\n",
      "Chunk 119: Length 676. Tokens 138\n",
      "Chunk 120: Length 773. Tokens 146\n",
      "Chunk 121: Length 925. Tokens 187\n",
      "Chunk 122: Length 535. Tokens 93\n",
      "Chunk 123: Length 523. Tokens 114\n",
      "Chunk 124: Length 929. Tokens 204\n",
      "Chunk 125: Length 376. Tokens 86\n",
      "Chunk 126: Length 703. Tokens 159\n",
      "Chunk 127: Length 842. Tokens 183\n",
      "Chunk 128: Length 781. Tokens 183\n",
      "Chunk 129: Length 969. Tokens 210\n",
      "Chunk 130: Length 559. Tokens 113\n",
      "Chunk 131: Length 1035. Tokens 243\n",
      "Chunk 132: Length 644. Tokens 146\n",
      "Chunk 133: Length 580. Tokens 123\n",
      "Chunk 134: Length 495. Tokens 108\n",
      "Chunk 135: Length 1072. Tokens 226\n",
      "Chunk 136: Length 680. Tokens 142\n",
      "Chunk 137: Length 849. Tokens 165\n",
      "Chunk 138: Length 734. Tokens 149\n",
      "Chunk 139: Length 920. Tokens 193\n",
      "Chunk 140: Length 631. Tokens 129\n",
      "Chunk 141: Length 990. Tokens 221\n",
      "Chunk 142: Length 982. Tokens 192\n",
      "Chunk 143: Length 937. Tokens 217\n",
      "Chunk 144: Length 972. Tokens 218\n",
      "Chunk 145: Length 542. Tokens 110\n",
      "Chunk 146: Length 909. Tokens 197\n",
      "Chunk 147: Length 962. Tokens 213\n",
      "Chunk 148: Length 940. Tokens 197\n",
      "Chunk 149: Length 577. Tokens 117\n",
      "Chunk 150: Length 938. Tokens 214\n",
      "Chunk 151: Length 694. Tokens 145\n",
      "Chunk 152: Length 930. Tokens 194\n",
      "Chunk 153: Length 399. Tokens 102\n",
      "Chunk 154: Length 897. Tokens 221\n",
      "Chunk 155: Length 819. Tokens 179\n",
      "Chunk 156: Length 786. Tokens 185\n",
      "Chunk 157: Length 900. Tokens 209\n",
      "Chunk 158: Length 878. Tokens 203\n",
      "Chunk 159: Length 1184. Tokens 260\n",
      "Chunk 160: Length 714. Tokens 157\n",
      "Chunk 161: Length 739. Tokens 141\n",
      "Chunk 162: Length 831. Tokens 184\n",
      "Chunk 163: Length 805. Tokens 167\n",
      "Chunk 164: Length 742. Tokens 144\n",
      "Chunk 165: Length 881. Tokens 188\n",
      "Chunk 166: Length 1091. Tokens 219\n",
      "Chunk 167: Length 836. Tokens 173\n",
      "Chunk 168: Length 595. Tokens 119\n",
      "Chunk 169: Length 377. Tokens 83\n",
      "Chunk 170: Length 813. Tokens 162\n",
      "Chunk 171: Length 503. Tokens 99\n",
      "Chunk 172: Length 616. Tokens 123\n",
      "Chunk 173: Length 535. Tokens 115\n",
      "Chunk 174: Length 947. Tokens 191\n",
      "Chunk 175: Length 692. Tokens 137\n",
      "Chunk 176: Length 763. Tokens 158\n",
      "Chunk 177: Length 924. Tokens 184\n",
      "Chunk 178: Length 747. Tokens 148\n",
      "Chunk 179: Length 563. Tokens 99\n",
      "Chunk 180: Length 633. Tokens 116\n",
      "Chunk 181: Length 661. Tokens 130\n",
      "Chunk 182: Length 735. Tokens 153\n",
      "Chunk 183: Length 923. Tokens 181\n",
      "Chunk 184: Length 740. Tokens 155\n",
      "Chunk 185: Length 685. Tokens 132\n",
      "Chunk 186: Length 398. Tokens 71\n",
      "Chunk 187: Length 1015. Tokens 215\n",
      "Chunk 188: Length 0. Tokens 0\n"
     ]
    }
   ],
   "source": [
    "with open('articles/chunks/chunks.txt', 'r') as file:\n",
    "  chunks = file.read()\n",
    "\n",
    "for i, chunk in enumerate(chunks.split('\\n')):\n",
    "  print(f\"Chunk {i}: Length {len(chunk)}. Tokens {num_tokens_from_string(chunk, 'cl100k_base')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tune-article",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
